{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# display all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document we will proceed to the import of the important of the raw datasets extracted from Vestiaire Collective's datalake. We will proceed data wrangling, data cleaning and data transformation to obtain a clean dataset ready to be used for the analysis. The final dataframe will be called \"df_model\" as it will be then used for the modelisation part."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMAO & TRANSACTION DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a part of the data\n",
    "\n",
    "df_mmao = pd.read_csv('Original_Data\\LIST_MMAO.csv', sep = '|')\n",
    "\n",
    "df_transaction = pd.read_csv('Original_Data\\LIST_TRANSACTION.csv', sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing_mmao_transac(df_mmao, df_transaction):\n",
    "    # Drop the columns with more than 15% null values\n",
    "    df = df_transaction.dropna(thresh=0.85*len(df_transaction), axis=1)\n",
    "\n",
    "    # Keeping only relevant features for our project\n",
    "    df_tri = df[['ID_PRODUCT',\n",
    "    'ID_ORDER',\n",
    "    'ID_CATEGORY',\n",
    "    'ID_SUBCATEGORY',\n",
    "    'ID_CONDITION',\n",
    "    'ID_BRAND',\n",
    "    'UNIVERSE',\n",
    "    'DEPOSIT_PRICE',\n",
    "    'PRICE_SOLD_GMV',\n",
    "    'NB_ITEMS',\n",
    "    'DISCOUNT_AMOUNT_GMV',\n",
    "    'DATE_SOLD',\n",
    "    'DATE_PUBLISHED',\n",
    "    'ID_PAYMENT_TYPE',\n",
    "    'ID_BUYER',\n",
    "    'RANK_BUYER',\n",
    "    'RANK_WITHIN_SEGMENT',\n",
    "    'ID_SEGMENT',\n",
    "    'LOVED',\n",
    "    'ORDER_VALUE_RANGE',\n",
    "    'BUYER_FEE_GMV',\n",
    "    'ORDER_MARKETING_CHANNEL',\n",
    "    'MMAO_PRICE_DROP',\n",
    "    'VOUCHER_REVENUE',\n",
    "    'BUYER_TYPE',\n",
    "    'FLAG_FRAUD',\n",
    "    'ID_GENDER',\n",
    "    'DATE_NEW_BUYER',\n",
    "    'DATE_CREATION',\n",
    "    'DATE_LAST_LOGIN',\n",
    "    'INACTIVE',\n",
    "    'NB_SOLD',\n",
    "    'VALUE_SOLD',\n",
    "    'NB_PUBLISHED',\n",
    "    'VALUE_PUBLISHED',\n",
    "    'NB_PURCHASED',\n",
    "    'VALUE_PURCHASED',\n",
    "    'NL_REACTIVITY_GROUP',\n",
    "    'ID_RFM_BUYER',\n",
    "    'RFM_BUYER',\n",
    "    'USER_SEGMENT',\n",
    "    'DATE_LAST_PURCHASE',\n",
    "    'DATE_FIRST_PURCHASE',\n",
    "    'BUYER_SEGMENT']]\n",
    "\n",
    "    # Creation of the two target variables \n",
    "    ##1. Is the buyer a repeater?\n",
    "    ### Convert buyer types to 0 and 1\n",
    "    df_tri['REPEATER'] = df_tri['BUYER_TYPE'].map({'New_Buyer': 0, 'Repeat': 1, 'Repeat_90D': 1})\n",
    "\n",
    "    ## 2. CLTV\n",
    "    ### Add the value purchased by the buyer and the value sold by the buyer\n",
    "    df_tri['CLTV'] = df_tri['VALUE_PURCHASED'] + df_tri['VALUE_SOLD']\n",
    "\n",
    "    # Drop the columns that are not needed anymore\n",
    "    df_transac_final = df_tri.drop(['BUYER_TYPE', 'VALUE_PURCHASED', 'VALUE_SOLD', 'VALUE_PUBLISHED'], axis=1)\n",
    "\n",
    "    # Create a feature with the number of offers grouped by each buyer\n",
    "    df_mmao['NB_OFFERS'] = df_mmao.groupby('ID_BUYER')['NB_TOTAL_OFFERS'].transform('sum')\n",
    "    df_mmao['AVG_OFFERS'] = df_mmao.groupby('ID_BUYER')['NB_TOTAL_OFFERS'].transform('mean')\n",
    "\n",
    "    # Drop the duplicates in mmao dataframe\n",
    "    df_mmao_final = df_mmao[['ID_BUYER', 'NB_OFFERS', 'AVG_OFFERS']].drop_duplicates()\n",
    "\n",
    "    # Merge the two dataframes\n",
    "\n",
    "    df_tr_mmao = pd.merge(df_transac_final, df_mmao_final, on='ID_BUYER', how='left')\n",
    "\n",
    "    # Fill null values due to the merge and drop the remaining values \n",
    "\n",
    "    df_tr_mmao['NB_OFFERS'].fillna(0, inplace=True)\n",
    "    df_tr_mmao['AVG_OFFERS'].fillna(0, inplace=True)\n",
    "\n",
    "    df_tr_mmao.dropna(inplace=True)\n",
    "\n",
    "    # Drop unrelevant features for the project \n",
    "    df_tr_mmao.drop(['ID_PRODUCT','ID_ORDER','UNIVERSE','LOVED','FLAG_FRAUD'], axis=1, inplace=True)\n",
    "\n",
    "    # Create feature number of days between date sold and date published, then drop the two columns\n",
    "\n",
    "    df_tr_mmao['DATE_SOLD'] = pd.to_datetime(df_tr_mmao['DATE_SOLD'])\n",
    "    df_tr_mmao['DATE_PUBLISHED'] = pd.to_datetime(df_tr_mmao['DATE_PUBLISHED'])\n",
    "    df_tr_mmao['NB_DAYS_ONLINE'] = (df_tr_mmao['DATE_SOLD'] - df_tr_mmao['DATE_PUBLISHED']).dt.days\n",
    "    df_tr_mmao.drop(['DATE_SOLD','DATE_PUBLISHED'], axis=1, inplace=True)\n",
    "\n",
    "    df_tr_mmao_test = df_tr_mmao.copy()\n",
    "\n",
    "    # Create aggregated features as we are grouping on ID_BUYER\n",
    "\n",
    "    ## Categorical or data that will be grouped by mode value\n",
    "    grouped = df_tr_mmao_test.groupby('ID_BUYER')\n",
    "    agg_mode = grouped.agg({'ID_CATEGORY': pd.Series.mode, 'ID_SUBCATEGORY': pd.Series.mode, 'ID_BRAND': pd.Series.mode,'ID_PAYMENT_TYPE': pd.Series.mode,'ORDER_MARKETING_CHANNEL': pd.Series.mode})\n",
    "    agg_mode= agg_mode.reset_index()\n",
    "    df_tr_mmao_test = df_tr_mmao_test.merge(agg_mode, on='ID_BUYER', how='left')\n",
    "    df_tr_mmao_test = df_tr_mmao_test.drop(['ID_CATEGORY_x', 'ID_SUBCATEGORY_x', 'ID_BRAND_x', 'ID_PAYMENT_TYPE_x', 'ORDER_MARKETING_CHANNEL_x'], axis=1)\n",
    "    df_tr_mmao_test = df_tr_mmao_test.rename(columns={'ID_CATEGORY_y': 'ID_CATEGORY','ID_SUBCATEGORY_y': 'ID_SUBCATEGORY','ID_BRAND_y': 'ID_BRAND','ID_PAYMENT_TYPE_y': 'ID_PAYMENT_TYPE', 'ORDER_MARKETING_CHANNEL_y': 'ORDER_MARKETING_CHANNEL'})\n",
    "\n",
    "    ## Numerical or data that will be grouped by mean value\n",
    "    agg_mean = df_tr_mmao_test.groupby('ID_BUYER').agg({'DEPOSIT_PRICE': 'mean', 'PRICE_SOLD_GMV': 'mean', 'NB_ITEMS': 'mean','DISCOUNT_AMOUNT_GMV': 'mean','BUYER_FEE_GMV': 'mean','MMAO_PRICE_DROP': 'mean', 'VOUCHER_REVENUE': 'mean', 'ID_CONDITION': 'mean', 'ID_GENDER': 'mean'})\n",
    "    agg_mean = agg_mean.reset_index()\n",
    "    df_tr_mmao_test = df_tr_mmao_test.merge(agg_mean, on='ID_BUYER', how='left')\n",
    "    df_tr_mmao_test = df_tr_mmao_test.drop(['DEPOSIT_PRICE_x', 'PRICE_SOLD_GMV_x', 'NB_ITEMS_x','DISCOUNT_AMOUNT_GMV_x','BUYER_FEE_GMV_x','MMAO_PRICE_DROP_x', 'VOUCHER_REVENUE_x','ID_CONDITION_x', 'ID_GENDER_x'], axis=1)\n",
    "    df_tr_mmao_test = df_tr_mmao_test.rename(columns={'DEPOSIT_PRICE_y': 'DEPOSIT_PRICE','PRICE_SOLD_GMV_y': 'PRICE_SOLD_GMV','NB_ITEMS_y': 'NB_ITEMS','DISCOUNT_AMOUNT_GMV_y': 'DISCOUNT_AMOUNT_GMV','BUYER_FEE_GMV_y': 'BUYER_FEE_GMV','MMAO_PRICE_DROP_y': 'MMAO_PRICE_DROP', 'VOUCHER_REVENUE_y': 'VOUCHER_REVENUE','ID_CONDITION_y': 'ID_CONDITION', 'ID_GENDER_y':'ID_GENDER' })\n",
    "\n",
    "    # Remap the gender values \n",
    "    df_tr_mmao_test.loc[df_tr_mmao_test['ID_GENDER'] == 0, 'ID_GENDER'] = 1\n",
    "    df_tr_mmao_test.loc[df_tr_mmao_test['ID_GENDER'] == 1, 'ID_GENDER'] = 0\n",
    "    df_tr_mmao_test.loc[df_tr_mmao_test['ID_GENDER'] == 2, 'ID_GENDER'] = 1\n",
    "    df_tr_mmao_test.loc[df_tr_mmao_test['ID_GENDER'] == 3, 'ID_GENDER'] = 2\n",
    "\n",
    "\n",
    "    df_final_v1 = df_tr_mmao_test.copy()\n",
    "\n",
    "    # create a dataframe with unique ID_BUYERS by grouping by ID_BUYER \n",
    "    df_final_v1 = df_final_v1.groupby('ID_BUYER').first().reset_index()\n",
    "\n",
    "    return df_final_v1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIKES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_likes = pd.read_csv('Original_Data\\LIST_LIKES.csv', sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing_likes(df_likes):\n",
    "\n",
    "    df = df_likes.dropna(thresh=0.85*len(df_likes), axis=1)\n",
    "\n",
    "    df2 = df[[\"ID_CLIENT\",\n",
    "            \"ID_BUYER\",\n",
    "            \"ID_LIKE\",\n",
    "            \"DATE_LIKED\",\n",
    "            \"DATE_PUBLISHED\",\n",
    "            \"UPDATEDAT\",\n",
    "            \"NBWISH\",\n",
    "            \"NB_CONSULTATION\",\n",
    "            \"MMAO_NB\",\n",
    "            \"LIKES\",\n",
    "            \"WISHES\",\n",
    "            \"LOVED\",\n",
    "            \"IS_ONLINE\",\n",
    "            \"IS_ACTIVE\",\n",
    "            \"IS_WITHDRAWN\",\n",
    "            \"IS_REJECTED\",\n",
    "            \"IS_RESERVED\",\n",
    "            \"STATUS_AFTER_7_DAYS\",\n",
    "            \"CURRENT_STATUS\",\n",
    "            \"ID_PRODUCT\",\n",
    "            \"ID_CATEGORY\",\n",
    "            \"ID_SUBCATEGORY\",\n",
    "            \"ID_BRAND\",\n",
    "            \"ID_UNIVERSE\",\n",
    "            \"ID_CONDITION\",\n",
    "            \"SEGMENT\",\n",
    "            \"ONED_SOLD_STATUS\",\n",
    "            \"TWOD_SOLD_STATUS\",\n",
    "            \"THREED_SOLD_STATUS\",\n",
    "            \"SEVEND_SOLD_STATUS\",\n",
    "            \"FIFTEEND_SOLD_STATUS\",\n",
    "            \"THIRTYD_SOLD_STATUS\",\n",
    "            \"NINETYD_SOLD_STATUS\",\n",
    "            \"ID_LAST_ACTION\",\n",
    "            \"IS_ITEM_WHITELISTED\",\n",
    "            \"IS_NEWIN_LIKED_IN_7DAYS\",\n",
    "            \"IS_NEWIN_WISHLISTED_IN_7DAYS\",\n",
    "            \"IS_NEWIN_MMAO_IN_7DAYS\",\n",
    "            \"IS_NEWIN_COMMENTED_IN_7DAYS\",\n",
    "            \"IS_NEWIN_ATC_IN_7DAYS\",\n",
    "            \"COMMISSION\"\n",
    "    ]]\n",
    "\n",
    "    df2 = df2.drop(['IS_ONLINE', 'IS_ACTIVE', 'IS_REJECTED','IS_RESERVED', 'IS_ITEM_WHITELISTED', 'ID_LAST_ACTION', 'NB_CONSULTATION',\n",
    "                    'ONED_SOLD_STATUS', 'TWOD_SOLD_STATUS', 'THREED_SOLD_STATUS', \n",
    "                    'SEVEND_SOLD_STATUS', 'FIFTEEND_SOLD_STATUS', 'THIRTYD_SOLD_STATUS',\n",
    "                    'NINETYD_SOLD_STATUS', 'IS_WITHDRAWN', 'IS_NEWIN_LIKED_IN_7DAYS',\n",
    "                    'IS_NEWIN_MMAO_IN_7DAYS', 'IS_NEWIN_WISHLISTED_IN_7DAYS', 'IS_NEWIN_COMMENTED_IN_7DAYS',\n",
    "                    'IS_NEWIN_ATC_IN_7DAYS', 'ID_UNIVERSE', 'LOVED', 'SEGMENT', \n",
    "                    'NBWISH', 'DATE_PUBLISHED', 'UPDATEDAT', \n",
    "                    'ID_BRAND', 'ID_CONDITION', 'ID_SUBCATEGORY', 'STATUS_AFTER_7_DAYS','CURRENT_STATUS','ID_CLIENT'], axis=1)\n",
    "\n",
    "    df2_test = df2.copy()\n",
    "\n",
    "    # Create features with the total number of likes, wishes and consultations grouped by each buyer\n",
    "    df2_test['Total_likes'] = df2_test.groupby('ID_BUYER')['LIKES'].transform('sum')\n",
    "    df2_test['Total_wishes'] = df2_test.groupby('ID_BUYER')['WISHES'].transform('sum')\n",
    "    df2_test['Total_MMAO_NB'] = df2_test.groupby('ID_BUYER')['MMAO_NB'].transform('sum')\n",
    "\n",
    "    # Create features with the unique number of commented products and categories grouped by each buyer\n",
    "    df2_test['NB_products_liked'] = df2_test.groupby('ID_BUYER')['ID_PRODUCT'].transform('nunique')\n",
    "    df2_test['NB_categories_liked'] = df2_test.groupby('ID_BUYER')['ID_CATEGORY'].transform('nunique')\n",
    "\n",
    "    # Create a feature with average commission grouped by each buyer\n",
    "    df2_test['Avg_commision'] = df2_test.groupby('ID_BUYER')['COMMISSION'].transform('mean')\n",
    "\n",
    "    df2_test['DATE_LIKED'] = pd.to_datetime(df2_test['DATE_LIKED'])\n",
    "\n",
    "    #Feature for the last comment made grouped by ID_BUYER\n",
    "    #recency is calculated based on difference of days from last comment ever made on platform\n",
    "    df_recency = df2_test.groupby(by='ID_BUYER',\n",
    "                            as_index=False)['DATE_LIKED'].max()\n",
    "    df_recency.columns = ['ID_BUYER', 'LastLikeDate']\n",
    "    df_recency['LastLikeDate'] = pd.to_datetime(df_recency['LastLikeDate'])\n",
    "    recent_date = df_recency['LastLikeDate'].max()\n",
    "    df_recency['Recency_liked'] = df_recency['LastLikeDate'].apply(\n",
    "        lambda x: (recent_date - x).days)\n",
    "    df_recency.head()\n",
    "\n",
    "\n",
    "    filtered_df = df2_test[~(df2_test['DATE_LIKED'] < '2022-01-01')]\n",
    "\n",
    "    frequency_df = filtered_df.groupby(\n",
    "        by=['ID_BUYER'], as_index=False)['DATE_LIKED'].count()\n",
    "    frequency_df.columns = ['ID_BUYER', 'Frequnecy_like_12M']\n",
    "    frequency_df.head()\n",
    "\n",
    "    df2_test2 = df2_test[['ID_BUYER', 'Total_likes', 'Total_wishes', 'Total_MMAO_NB', 'NB_products_liked', 'NB_categories_liked', 'Avg_commision']].drop_duplicates()\n",
    "\n",
    "    df2_with_recency = df2_test2.merge(df_recency, on='ID_BUYER')\n",
    "\n",
    "    df2_with_recency_and_frquency = df2_with_recency.merge(frequency_df, on='ID_BUYER',how='left')\n",
    "\n",
    "    df2_with_recency_and_frquency['Frequnecy_like_12M'].fillna(0, inplace=True)\n",
    "\n",
    "    return df2_with_recency_and_frquency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMMENTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment = pd.read_csv('Original_Data\\LIST_COMMENT.csv', sep = '|',  nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing_comment(df_comment):\n",
    "    # Drop the columns with more than 20% null values\n",
    "    df = df_comment.dropna(thresh=0.80*len(df_comment), axis=1)\n",
    "\n",
    "    df2 = df[[\"ID_CLIENT\",\n",
    "            \"ID_BUYER\",\n",
    "            \"ID_COMMENT\",\n",
    "            \"DATE_COMMENT\",\n",
    "            \"DATE_PUBLISHED\",\n",
    "            \"UPDATEDAT\",\n",
    "            \"NB_LIKES\",\n",
    "            \"NBWISH\",\n",
    "            \"NB_CONSULTATION\",\n",
    "            \"LIKES\",\n",
    "            \"WISHES\",\n",
    "            \"LOVED\",\n",
    "            \"IS_ONLINE\",\n",
    "            \"IS_ACTIVE\",\n",
    "            \"IS_WITHDRAWN\",\n",
    "            \"IS_REJECTED\",\n",
    "            \"IS_RESERVED\",\n",
    "            \"STATUS_AFTER_7_DAYS\",\n",
    "            \"CURRENT_STATUS\",\n",
    "            \"ACCEPTED_BY\",\n",
    "            \"CURATOR_TYPE\",\n",
    "            \"ID_PRODUCT\",\n",
    "            \"ID_CATEGORY\",\n",
    "            \"ID_SUBCATEGORY\",\n",
    "            \"ID_BRAND\",\n",
    "            \"UNIVERSE\",\n",
    "            \"ID_CONDITION\",\n",
    "            \"SEGMENT\",\n",
    "            \"ONED_SOLD_STATUS\",\n",
    "            \"TWOD_SOLD_STATUS\",\n",
    "            \"THREED_SOLD_STATUS\",\n",
    "            \"SEVEND_SOLD_STATUS\",\n",
    "            \"FIFTEEND_SOLD_STATUS\",\n",
    "            \"THIRTYD_SOLD_STATUS\",\n",
    "            \"NINETYD_SOLD_STATUS\",\n",
    "            \"ID_LAST_ACTION\",\n",
    "            \"IS_ITEM_WHITELISTED\",\n",
    "            \"IS_NEWIN_LIKED_IN_7DAYS\",\n",
    "            \"IS_NEWIN_WISHLISTED_IN_7DAYS\",\n",
    "            \"IS_NEWIN_MMAO_IN_7DAYS\",\n",
    "            \"IS_NEWIN_COMMENTED_IN_7DAYS\",\n",
    "            \"IS_NEWIN_ATC_IN_7DAYS\",\n",
    "            \"COMMISSION\"\n",
    "    ]]\n",
    "\n",
    "    #dropping irrelevant columns\n",
    "    df2 = df2.drop(['CURATOR_TYPE', 'IS_ONLINE', 'IS_ACTIVE', 'IS_REJECTED','IS_RESERVED', \n",
    "                    'IS_ITEM_WHITELISTED','ACCEPTED_BY', 'ID_LAST_ACTION', 'NB_CONSULTATION'\n",
    "                    'ONED_SOLD_STATUS', 'TWOD_SOLD_STATUS', 'THREED_SOLD_STATUS', \n",
    "                    'SEVEND_SOLD_STATUS', 'FIFTEEND_SOLD_STATUS', 'THIRTYD_SOLD_STATUS',\n",
    "                    'NINETYD_SOLD_STATUS', 'IS_WITHDRAWN', 'IS_NEWIN_LIKED_IN_7DAYS',\n",
    "                    'IS_NEWIN_MMAO_IN_7DAYS', 'IS_NEWIN_WISHLISTED_IN_7DAYS', 'IS_NEWIN_COMMENTED_IN_7DAYS',\n",
    "                    'IS_NEWIN_ATC_IN_7DAYS', 'UNIVERSE', 'LOVED', 'SEGMENT', \n",
    "                    'LIKES', 'WISHES', 'DATE_PUBLISHED', 'UPDATEDAT', \n",
    "                    'ID_BRAND', 'ID_CONDITION', 'ID_SUBCATEGORY', 'STATUS_AFTER_7_DAYS','CURRENT_STATUS','ID_CLIENT'], axis=1)\n",
    "\n",
    "    df2_test = df2.copy()\n",
    "\n",
    "    # Create features with the total number of likes, wishes and consultations grouped by each buyer\n",
    "    df2_test['Total_nb_likes'] = df2_test.groupby('ID_BUYER')['NB_LIKES'].transform('sum')\n",
    "    df2_test['Total_nb_wish'] = df2_test.groupby('ID_BUYER')['NBWISH'].transform('sum')\n",
    "\n",
    "    # Create features with the unique number of commented products and categories grouped by each buyer\n",
    "    df2_test['NB_products_commented'] = df2_test.groupby('ID_BUYER')['ID_PRODUCT'].transform('nunique')\n",
    "    df2_test['NB_categories_commented'] = df2_test.groupby('ID_BUYER')['ID_CATEGORY'].transform('nunique')\n",
    "\n",
    "    # Create a feature with average commission grouped by each buyer\n",
    "    df2_test['Avg_commision'] = df2_test.groupby('ID_BUYER')['COMMISSION'].transform('mean')\n",
    "\n",
    "    df2_test['DATE_COMMENT'] = pd.to_datetime(df2_test['DATE_COMMENT'])\n",
    "\n",
    "    #Feature for the last comment made grouped by ID_BUYER\n",
    "    #recency is calculated based on difference of days from last comment ever made on platform\n",
    "    df_recency = df2_test.groupby(by='ID_BUYER',\n",
    "                            as_index=False)['DATE_COMMENT'].max()\n",
    "    df_recency.columns = ['ID_BUYER', 'LastCommentDate']\n",
    "    df_recency['LastCommentDate'] = pd.to_datetime(df_recency['LastCommentDate'])\n",
    "    recent_date = df_recency['LastCommentDate'].max()\n",
    "    df_recency['Recency_comment'] = df_recency['LastCommentDate'].apply(\n",
    "        lambda x: (recent_date - x).days)\n",
    "    df_recency.head()\n",
    "\n",
    "    filtered_df = df2_test[~(df2_test['DATE_COMMENT'] < '2022-01-01')]\n",
    "\n",
    "    frequency_df = filtered_df.groupby(\n",
    "        by=['ID_BUYER'], as_index=False)['DATE_COMMENT'].count()\n",
    "    frequency_df.columns = ['ID_BUYER', 'Frequnecy_comment_12M']\n",
    "\n",
    "    df2_test2 = df2_test[['ID_BUYER', 'Total_nb_likes', 'Total_nb_wish', 'NB_products_commented', 'NB_categories_commented', 'Avg_commision']].drop_duplicates()\n",
    "\n",
    "    df2_with_recency = df2_test2.merge(df_recency, on='ID_BUYER')\n",
    "\n",
    "    df2_with_recency_and_frquency = df2_with_recency.merge(frequency_df, on='ID_BUYER',how='left')\n",
    "\n",
    "    df2_with_recency_and_frquency['Frequnecy_comment_12M'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "    return df2_with_recency_and_frquency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the three dataframes for the final model\n",
    "\n",
    "def merge_dataframes(df_transac, df_likes_final, df_comments_final):\n",
    "    # Merge the dataframes\n",
    "    df_merge = pd.merge(df_transac, df_likes_final, on='ID_BUYER', how='left')\n",
    "    df_merge = pd.merge(df_merge, df_comments_final, on='ID_BUYER', how='left')\n",
    "\n",
    "    # Export as csv\n",
    "\n",
    "    df_merge.to_csv(\"df_model_v4.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main call to the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transac = data_processing_mmao_transac(df_mmao, df_transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_likes_final = data_processing_likes(df_likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_final = data_preprocessing_comment(df_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.63 GiB for an array with shape (110, 3208973) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24296\\2604005348.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf_transac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_processing_mmao_transac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_mmao\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_transaction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_likes_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_processing_likes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_likes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_comments_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_preprocessing_comment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_comment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmerge_dataframes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_transac\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_likes_final\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_comments_final\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24296\\2791205041.py\u001b[0m in \u001b[0;36mdata_processing_likes\u001b[1;34m(df_likes)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdata_processing_likes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_likes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_likes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.85\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_likes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     df2 = df[[\"ID_CLIENT\",\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 )\n\u001b[1;32m--> 331\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdropna\u001b[1;34m(self, axis, how, thresh, subset, inplace)\u001b[0m\n\u001b[0;32m   6577\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6578\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6579\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1073\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1290\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getbool_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1091\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m         \u001b[0minds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1093\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1094\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   3900\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3901\u001b[0m         \"\"\"\n\u001b[1;32m-> 3902\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3903\u001b[0m         \u001b[1;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3904\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_take\u001b[1;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[0;32m   3884\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3885\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3886\u001b[1;33m         new_data = self._mgr.take(\n\u001b[0m\u001b[0;32m   3887\u001b[0m             \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3888\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert_indices)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m         return self.reindex_indexer(\n\u001b[0m\u001b[0;32m    979\u001b[0m             \u001b[0mnew_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m             \u001b[0mindexer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 743\u001b[1;33m             new_blocks, new_refs = self._slice_take_blocks_ax0(\n\u001b[0m\u001b[0;32m    744\u001b[0m                 \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    910\u001b[0m                             \u001b[0mrefs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m                         \u001b[0mnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_nd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtaker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m                         \u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m                         \u001b[0mrefs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[1;31m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         new_values = algos.take_nd(\n\u001b[0m\u001b[0;32m    881\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m         )\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\array_algos\\take.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_take_nd_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\array_algos\\take.py\u001b[0m in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"F\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     func = _get_take_nd_function(\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.63 GiB for an array with shape (110, 3208973) and data type float64"
     ]
    }
   ],
   "source": [
    "merge_dataframes(df_transac, df_likes_final, df_comments_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
