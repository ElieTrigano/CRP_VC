{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# display all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document we will proceed to the import of the important of the raw datasets extracted from Vestiaire Collective's datalake. We will proceed data wrangling, data cleaning and data transformation to obtain a clean dataset ready to be used for the analysis. The final dataframe will be called \"df_model\" as it will be then used for the modelisation part."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMAO & TRANSACTION DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a part of the data\n",
    "\n",
    "df_mmao = pd.read_csv('Original_Data\\LIST_MMAO.csv', sep = '|')\n",
    "\n",
    "df_transaction = pd.read_csv('Original_Data\\LIST_TRANSACTION.csv', sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing_mmao_transac(df_mmao, df_transaction):\n",
    "    # Drop the columns with more than 15% null values\n",
    "    df = df_transaction.dropna(thresh=0.85*len(df_transaction), axis=1)\n",
    "\n",
    "    # Keeping only relevant features for our project\n",
    "    df_tri = df[['ID_PRODUCT',\n",
    "    'ID_ORDER',\n",
    "    'ID_CATEGORY',\n",
    "    'ID_SUBCATEGORY',\n",
    "    'ID_CONDITION',\n",
    "    'ID_BRAND',\n",
    "    'UNIVERSE',\n",
    "    'DEPOSIT_PRICE',\n",
    "    'PRICE_SOLD_GMV',\n",
    "    'NB_ITEMS',\n",
    "    'DISCOUNT_AMOUNT_GMV',\n",
    "    'DATE_SOLD',\n",
    "    'DATE_PUBLISHED',\n",
    "    'ID_PAYMENT_TYPE',\n",
    "    'ID_BUYER',\n",
    "    'RANK_BUYER',\n",
    "    'RANK_WITHIN_SEGMENT',\n",
    "    'ID_SEGMENT',\n",
    "    'LOVED',\n",
    "    'ORDER_VALUE_RANGE',\n",
    "    'BUYER_FEE_GMV',\n",
    "    'ORDER_MARKETING_CHANNEL',\n",
    "    'MMAO_PRICE_DROP',\n",
    "    'VOUCHER_REVENUE',\n",
    "    'BUYER_TYPE',\n",
    "    'FLAG_FRAUD',\n",
    "    'ID_GENDER',\n",
    "    'DATE_NEW_BUYER',\n",
    "    'DATE_CREATION',\n",
    "    'DATE_LAST_LOGIN',\n",
    "    'INACTIVE',\n",
    "    'NB_SOLD',\n",
    "    'VALUE_SOLD',\n",
    "    'NB_PUBLISHED',\n",
    "    'VALUE_PUBLISHED',\n",
    "    'NB_PURCHASED',\n",
    "    'VALUE_PURCHASED',\n",
    "    'NL_REACTIVITY_GROUP',\n",
    "    'ID_RFM_BUYER',\n",
    "    'RFM_BUYER',\n",
    "    'USER_SEGMENT',\n",
    "    'DATE_LAST_PURCHASE',\n",
    "    'DATE_FIRST_PURCHASE',\n",
    "    'BUYER_SEGMENT']]\n",
    "\n",
    "    # Creation of the two target variables \n",
    "    ##1. Is the buyer a repeater?\n",
    "    ### Convert buyer types to 0 and 1\n",
    "    df_tri['REPEATER'] = df_tri['BUYER_TYPE'].map({'New_Buyer': 0, 'Repeat': 1, 'Repeat_90D': 1})\n",
    "\n",
    "    ## 2. CLTV\n",
    "    ### Add the value purchased by the buyer and the value sold by the buyer\n",
    "    df_tri['CLTV'] = df_tri['VALUE_PURCHASED'] + df_tri['VALUE_SOLD']\n",
    "\n",
    "    # Drop the columns that are not needed anymore\n",
    "    df_transac_final = df_tri.drop(['BUYER_TYPE', 'VALUE_PURCHASED', 'VALUE_SOLD', 'VALUE_PUBLISHED'], axis=1)\n",
    "\n",
    "    # Create a feature with the number of offers grouped by each buyer\n",
    "    df_mmao['NB_OFFERS'] = df_mmao.groupby('ID_BUYER')['NB_TOTAL_OFFERS'].transform('sum')\n",
    "    df_mmao['AVG_OFFERS'] = df_mmao.groupby('ID_BUYER')['NB_TOTAL_OFFERS'].transform('mean')\n",
    "\n",
    "    # Drop the duplicates in mmao dataframe\n",
    "    df_mmao_final = df_mmao[['ID_BUYER', 'NB_OFFERS', 'AVG_OFFERS']].drop_duplicates()\n",
    "\n",
    "    # Merge the two dataframes\n",
    "\n",
    "    df_tr_mmao = pd.merge(df_transac_final, df_mmao_final, on='ID_BUYER', how='left')\n",
    "\n",
    "    # Fill null values due to the merge and drop the remaining values \n",
    "\n",
    "    df_tr_mmao['NB_OFFERS'].fillna(0, inplace=True)\n",
    "    df_tr_mmao['AVG_OFFERS'].fillna(0, inplace=True)\n",
    "\n",
    "    df_tr_mmao.dropna(inplace=True)\n",
    "\n",
    "    # Drop unrelevant features for the project \n",
    "    df_tr_mmao.drop(['ID_PRODUCT','ID_ORDER','UNIVERSE','LOVED','FLAG_FRAUD'], axis=1, inplace=True)\n",
    "\n",
    "    # Create feature number of days between date sold and date published, then drop the two columns\n",
    "\n",
    "    df_tr_mmao['DATE_SOLD'] = pd.to_datetime(df_tr_mmao['DATE_SOLD'])\n",
    "    df_tr_mmao['DATE_PUBLISHED'] = pd.to_datetime(df_tr_mmao['DATE_PUBLISHED'])\n",
    "    df_tr_mmao['NB_DAYS_ONLINE'] = (df_tr_mmao['DATE_SOLD'] - df_tr_mmao['DATE_PUBLISHED']).dt.days\n",
    "    df_tr_mmao.drop(['DATE_SOLD','DATE_PUBLISHED'], axis=1, inplace=True)\n",
    "\n",
    "    df_tr_mmao_test = df_tr_mmao.copy()\n",
    "\n",
    "    # Create aggregated features as we are grouping on ID_BUYER\n",
    "\n",
    "    ## Categorical or data that will be grouped by mode value\n",
    "    grouped = df_tr_mmao_test.groupby('ID_BUYER')\n",
    "    agg_mode = grouped.agg({'ID_CATEGORY': pd.Series.mode, 'ID_SUBCATEGORY': pd.Series.mode, 'ID_BRAND': pd.Series.mode,'ID_PAYMENT_TYPE': pd.Series.mode,'ORDER_MARKETING_CHANNEL': pd.Series.mode})\n",
    "    agg_mode= agg_mode.reset_index()\n",
    "    df_tr_mmao_test = df_tr_mmao_test.merge(agg_mode, on='ID_BUYER', how='left')\n",
    "    df_tr_mmao_test = df_tr_mmao_test.drop(['ID_CATEGORY_x', 'ID_SUBCATEGORY_x', 'ID_BRAND_x', 'ID_PAYMENT_TYPE_x', 'ORDER_MARKETING_CHANNEL_x'], axis=1)\n",
    "    df_tr_mmao_test = df_tr_mmao_test.rename(columns={'ID_CATEGORY_y': 'ID_CATEGORY','ID_SUBCATEGORY_y': 'ID_SUBCATEGORY','ID_BRAND_y': 'ID_BRAND','ID_PAYMENT_TYPE_y': 'ID_PAYMENT_TYPE', 'ORDER_MARKETING_CHANNEL_y': 'ORDER_MARKETING_CHANNEL'})\n",
    "\n",
    "    ## Numerical or data that will be grouped by mean value\n",
    "    agg_mean = df_tr_mmao_test.groupby('ID_BUYER').agg({'DEPOSIT_PRICE': 'mean', 'PRICE_SOLD_GMV': 'mean', 'NB_ITEMS': 'mean','DISCOUNT_AMOUNT_GMV': 'mean','BUYER_FEE_GMV': 'mean','MMAO_PRICE_DROP': 'mean', 'VOUCHER_REVENUE': 'mean', 'ID_CONDITION': 'mean', 'ID_GENDER': 'mean'})\n",
    "    agg_mean = agg_mean.reset_index()\n",
    "    df_tr_mmao_test = df_tr_mmao_test.merge(agg_mean, on='ID_BUYER', how='left')\n",
    "    df_tr_mmao_test = df_tr_mmao_test.drop(['DEPOSIT_PRICE_x', 'PRICE_SOLD_GMV_x', 'NB_ITEMS_x','DISCOUNT_AMOUNT_GMV_x','BUYER_FEE_GMV_x','MMAO_PRICE_DROP_x', 'VOUCHER_REVENUE_x','ID_CONDITION_x', 'ID_GENDER_x'], axis=1)\n",
    "    df_tr_mmao_test = df_tr_mmao_test.rename(columns={'DEPOSIT_PRICE_y': 'DEPOSIT_PRICE','PRICE_SOLD_GMV_y': 'PRICE_SOLD_GMV','NB_ITEMS_y': 'NB_ITEMS','DISCOUNT_AMOUNT_GMV_y': 'DISCOUNT_AMOUNT_GMV','BUYER_FEE_GMV_y': 'BUYER_FEE_GMV','MMAO_PRICE_DROP_y': 'MMAO_PRICE_DROP', 'VOUCHER_REVENUE_y': 'VOUCHER_REVENUE','ID_CONDITION_y': 'ID_CONDITION', 'ID_GENDER_y':'ID_GENDER' })\n",
    "\n",
    "    # Remap the gender values \n",
    "    df_tr_mmao_test.loc[df_tr_mmao_test['ID_GENDER'] == 0, 'ID_GENDER'] = 1\n",
    "    df_tr_mmao_test.loc[df_tr_mmao_test['ID_GENDER'] == 1, 'ID_GENDER'] = 0\n",
    "    df_tr_mmao_test.loc[df_tr_mmao_test['ID_GENDER'] == 2, 'ID_GENDER'] = 1\n",
    "    df_tr_mmao_test.loc[df_tr_mmao_test['ID_GENDER'] == 3, 'ID_GENDER'] = 2\n",
    "\n",
    "\n",
    "    df_final_v1 = df_tr_mmao_test.copy()\n",
    "\n",
    "    # create a dataframe with unique ID_BUYERS by grouping by ID_BUYER \n",
    "    df_final_v1 = df_final_v1.groupby('ID_BUYER').first().reset_index()\n",
    "\n",
    "    return df_final_v1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIKES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_likes = pd.read_csv('Original_Data\\LIST_LIKES.csv', sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing_likes(df_likes):\n",
    "\n",
    "    df = df_likes.dropna(thresh=0.85*len(df_likes), axis=1)\n",
    "\n",
    "    df2 = df[[\"ID_CLIENT\",\n",
    "            \"ID_BUYER\",\n",
    "            \"ID_LIKE\",\n",
    "            \"DATE_LIKED\",\n",
    "            \"DATE_PUBLISHED\",\n",
    "            \"UPDATEDAT\",\n",
    "            \"NBWISH\",\n",
    "            \"NB_CONSULTATION\",\n",
    "            \"MMAO_NB\",\n",
    "            \"LIKES\",\n",
    "            \"WISHES\",\n",
    "            \"LOVED\",\n",
    "            \"IS_ONLINE\",\n",
    "            \"IS_ACTIVE\",\n",
    "            \"IS_WITHDRAWN\",\n",
    "            \"IS_REJECTED\",\n",
    "            \"IS_RESERVED\",\n",
    "            \"STATUS_AFTER_7_DAYS\",\n",
    "            \"CURRENT_STATUS\",\n",
    "            \"ID_PRODUCT\",\n",
    "            \"ID_CATEGORY\",\n",
    "            \"ID_SUBCATEGORY\",\n",
    "            \"ID_BRAND\",\n",
    "            \"ID_UNIVERSE\",\n",
    "            \"ID_CONDITION\",\n",
    "            \"SEGMENT\",\n",
    "            \"ONED_SOLD_STATUS\",\n",
    "            \"TWOD_SOLD_STATUS\",\n",
    "            \"THREED_SOLD_STATUS\",\n",
    "            \"SEVEND_SOLD_STATUS\",\n",
    "            \"FIFTEEND_SOLD_STATUS\",\n",
    "            \"THIRTYD_SOLD_STATUS\",\n",
    "            \"NINETYD_SOLD_STATUS\",\n",
    "            \"ID_LAST_ACTION\",\n",
    "            \"IS_ITEM_WHITELISTED\",\n",
    "            \"IS_NEWIN_LIKED_IN_7DAYS\",\n",
    "            \"IS_NEWIN_WISHLISTED_IN_7DAYS\",\n",
    "            \"IS_NEWIN_MMAO_IN_7DAYS\",\n",
    "            \"IS_NEWIN_COMMENTED_IN_7DAYS\",\n",
    "            \"IS_NEWIN_ATC_IN_7DAYS\",\n",
    "            \"COMMISSION\"\n",
    "    ]]\n",
    "\n",
    "    df2 = df2.drop(['IS_ONLINE', 'IS_ACTIVE', 'IS_REJECTED','IS_RESERVED', 'IS_ITEM_WHITELISTED', 'ID_LAST_ACTION', 'NB_CONSULTATION',\n",
    "                    'ONED_SOLD_STATUS', 'TWOD_SOLD_STATUS', 'THREED_SOLD_STATUS', \n",
    "                    'SEVEND_SOLD_STATUS', 'FIFTEEND_SOLD_STATUS', 'THIRTYD_SOLD_STATUS',\n",
    "                    'NINETYD_SOLD_STATUS', 'IS_WITHDRAWN', 'IS_NEWIN_LIKED_IN_7DAYS',\n",
    "                    'IS_NEWIN_MMAO_IN_7DAYS', 'IS_NEWIN_WISHLISTED_IN_7DAYS', 'IS_NEWIN_COMMENTED_IN_7DAYS',\n",
    "                    'IS_NEWIN_ATC_IN_7DAYS', 'ID_UNIVERSE', 'LOVED', 'SEGMENT', \n",
    "                    'NBWISH', 'DATE_PUBLISHED', 'UPDATEDAT', \n",
    "                    'ID_BRAND', 'ID_CONDITION', 'ID_SUBCATEGORY', 'STATUS_AFTER_7_DAYS','CURRENT_STATUS','ID_CLIENT'], axis=1)\n",
    "\n",
    "    df2_test = df2.copy()\n",
    "\n",
    "    # Create features with the total number of likes, wishes and consultations grouped by each buyer\n",
    "    df2_test['Total_likes'] = df2_test.groupby('ID_BUYER')['LIKES'].transform('sum')\n",
    "    df2_test['Total_wishes'] = df2_test.groupby('ID_BUYER')['WISHES'].transform('sum')\n",
    "    df2_test['Total_MMAO_NB'] = df2_test.groupby('ID_BUYER')['MMAO_NB'].transform('sum')\n",
    "\n",
    "    # Create features with the unique number of commented products and categories grouped by each buyer\n",
    "    df2_test['NB_products_liked'] = df2_test.groupby('ID_BUYER')['ID_PRODUCT'].transform('nunique')\n",
    "    df2_test['NB_categories_liked'] = df2_test.groupby('ID_BUYER')['ID_CATEGORY'].transform('nunique')\n",
    "\n",
    "    # Create a feature with average commission grouped by each buyer\n",
    "    df2_test['Avg_commision'] = df2_test.groupby('ID_BUYER')['COMMISSION'].transform('mean')\n",
    "\n",
    "    df2_test['DATE_LIKED'] = pd.to_datetime(df2_test['DATE_LIKED'])\n",
    "\n",
    "    #Feature for the last comment made grouped by ID_BUYER\n",
    "    #recency is calculated based on difference of days from last comment ever made on platform\n",
    "    df_recency = df2_test.groupby(by='ID_BUYER',\n",
    "                            as_index=False)['DATE_LIKED'].max()\n",
    "    df_recency.columns = ['ID_BUYER', 'LastLikeDate']\n",
    "    df_recency['LastLikeDate'] = pd.to_datetime(df_recency['LastLikeDate'])\n",
    "    recent_date = df_recency['LastLikeDate'].max()\n",
    "    df_recency['Recency_liked'] = df_recency['LastLikeDate'].apply(\n",
    "        lambda x: (recent_date - x).days)\n",
    "    df_recency.head()\n",
    "\n",
    "\n",
    "    filtered_df = df2_test[~(df2_test['DATE_LIKED'] < '2022-01-01')]\n",
    "\n",
    "    frequency_df = filtered_df.groupby(\n",
    "        by=['ID_BUYER'], as_index=False)['DATE_LIKED'].count()\n",
    "    frequency_df.columns = ['ID_BUYER', 'Frequnecy_like_12M']\n",
    "    frequency_df.head()\n",
    "\n",
    "    df2_test2 = df2_test[['ID_BUYER', 'Total_likes', 'Total_wishes', 'Total_MMAO_NB', 'NB_products_liked', 'NB_categories_liked', 'Avg_commision']].drop_duplicates()\n",
    "\n",
    "    df2_with_recency = df2_test2.merge(df_recency, on='ID_BUYER')\n",
    "\n",
    "    df2_with_recency_and_frquency = df2_with_recency.merge(frequency_df, on='ID_BUYER',how='left')\n",
    "\n",
    "    df2_with_recency_and_frquency['Frequnecy_like_12M'].fillna(0, inplace=True)\n",
    "\n",
    "    return df2_with_recency_and_frquency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMMENTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment = pd.read_csv('Original_Data\\LIST_COMMENT.csv', sep = '|',  nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing_comment(df_comment):\n",
    "    # Drop the columns with more than 20% null values\n",
    "    df = df_comment.dropna(thresh=0.80*len(df_comment), axis=1)\n",
    "\n",
    "    df2 = df[[\"ID_CLIENT\",\n",
    "            \"ID_BUYER\",\n",
    "            \"ID_COMMENT\",\n",
    "            \"DATE_COMMENT\",\n",
    "            \"DATE_PUBLISHED\",\n",
    "            \"UPDATEDAT\",\n",
    "            \"NB_LIKES\",\n",
    "            \"NBWISH\",\n",
    "            \"NB_CONSULTATION\",\n",
    "            \"LIKES\",\n",
    "            \"WISHES\",\n",
    "            \"LOVED\",\n",
    "            \"IS_ONLINE\",\n",
    "            \"IS_ACTIVE\",\n",
    "            \"IS_WITHDRAWN\",\n",
    "            \"IS_REJECTED\",\n",
    "            \"IS_RESERVED\",\n",
    "            \"STATUS_AFTER_7_DAYS\",\n",
    "            \"CURRENT_STATUS\",\n",
    "            \"ACCEPTED_BY\",\n",
    "            \"CURATOR_TYPE\",\n",
    "            \"ID_PRODUCT\",\n",
    "            \"ID_CATEGORY\",\n",
    "            \"ID_SUBCATEGORY\",\n",
    "            \"ID_BRAND\",\n",
    "            \"UNIVERSE\",\n",
    "            \"ID_CONDITION\",\n",
    "            \"SEGMENT\",\n",
    "            \"ONED_SOLD_STATUS\",\n",
    "            \"TWOD_SOLD_STATUS\",\n",
    "            \"THREED_SOLD_STATUS\",\n",
    "            \"SEVEND_SOLD_STATUS\",\n",
    "            \"FIFTEEND_SOLD_STATUS\",\n",
    "            \"THIRTYD_SOLD_STATUS\",\n",
    "            \"NINETYD_SOLD_STATUS\",\n",
    "            \"ID_LAST_ACTION\",\n",
    "            \"IS_ITEM_WHITELISTED\",\n",
    "            \"IS_NEWIN_LIKED_IN_7DAYS\",\n",
    "            \"IS_NEWIN_WISHLISTED_IN_7DAYS\",\n",
    "            \"IS_NEWIN_MMAO_IN_7DAYS\",\n",
    "            \"IS_NEWIN_COMMENTED_IN_7DAYS\",\n",
    "            \"IS_NEWIN_ATC_IN_7DAYS\",\n",
    "            \"COMMISSION\"\n",
    "    ]]\n",
    "\n",
    "#dropping irrelevant columns\n",
    "    df2 = df2.drop(['CURATOR_TYPE', 'IS_ONLINE', 'IS_ACTIVE', 'IS_REJECTED','IS_RESERVED', 'IS_ITEM_WHITELISTED','ACCEPTED_BY', 'ID_LAST_ACTION', 'NB_CONSULTATION'], axis=1)\n",
    "    df2 = df2.drop(['ONED_SOLD_STATUS', 'TWOD_SOLD_STATUS', 'THREED_SOLD_STATUS', \n",
    "                'SEVEND_SOLD_STATUS', 'FIFTEEND_SOLD_STATUS', 'THIRTYD_SOLD_STATUS',\n",
    "                'NINETYD_SOLD_STATUS', 'IS_WITHDRAWN', 'IS_NEWIN_LIKED_IN_7DAYS',\n",
    "                'IS_NEWIN_MMAO_IN_7DAYS', 'IS_NEWIN_WISHLISTED_IN_7DAYS', 'IS_NEWIN_COMMENTED_IN_7DAYS',\n",
    "                'IS_NEWIN_ATC_IN_7DAYS', 'UNIVERSE', 'LOVED', 'SEGMENT', \n",
    "                'LIKES', 'WISHES', 'DATE_PUBLISHED', 'UPDATEDAT', \n",
    "                'ID_BRAND', 'ID_CONDITION', 'ID_SUBCATEGORY', 'STATUS_AFTER_7_DAYS','CURRENT_STATUS'], axis=1)\n",
    "    \n",
    "    df2 = df2.drop(['ID_CLIENT'], axis=1)\n",
    "    df2_test = df2.copy()\n",
    "\n",
    "    # Create features with the total number of likes, wishes and consultations grouped by each buyer\n",
    "    df2_test['Total_nb_likes'] = df2_test.groupby('ID_BUYER')['NB_LIKES'].transform('sum')\n",
    "    df2_test['Total_nb_wish'] = df2_test.groupby('ID_BUYER')['NBWISH'].transform('sum')\n",
    "\n",
    "    # Create features with the unique number of commented products and categories grouped by each buyer\n",
    "    df2_test['NB_products_commented'] = df2_test.groupby('ID_BUYER')['ID_PRODUCT'].transform('nunique')\n",
    "    df2_test['NB_categories_commented'] = df2_test.groupby('ID_BUYER')['ID_CATEGORY'].transform('nunique')\n",
    "\n",
    "    # Create a feature with average commission grouped by each buyer\n",
    "    df2_test['Avg_commision'] = df2_test.groupby('ID_BUYER')['COMMISSION'].transform('mean')\n",
    "\n",
    "    df2_test['DATE_COMMENT'] = pd.to_datetime(df2_test['DATE_COMMENT'])\n",
    "\n",
    "    #Feature for the last comment made grouped by ID_BUYER\n",
    "    #recency is calculated based on difference of days from last comment ever made on platform\n",
    "    df_recency = df.groupby(by='ID_BUYER',\n",
    "                            as_index=False)['DATE_COMMENT'].max()\n",
    "    df_recency.columns = ['ID_BUYER', 'LastCommentDate']\n",
    "    df_recency['LastCommentDate'] = pd.to_datetime(df_recency['LastCommentDate'])\n",
    "    recent_date = df_recency['LastCommentDate'].max()\n",
    "    df_recency['Recency_comment'] = df_recency['LastCommentDate'].apply(\n",
    "        lambda x: (recent_date - x).days)\n",
    "\n",
    "    filtered_df = df2_test[~(df2_test['DATE_COMMENT'] < '2022-01-01')]\n",
    "\n",
    "    frequency_df = filtered_df.groupby(\n",
    "        by=['ID_BUYER'], as_index=False)['DATE_COMMENT'].count()\n",
    "    frequency_df.columns = ['ID_BUYER', 'Frequnecy_comment_12M']\n",
    "\n",
    "    df2_test2 = df2_test[['ID_BUYER', 'Total_nb_likes', 'Total_nb_wish', 'NB_products_commented', 'NB_categories_commented', 'Avg_commision']].drop_duplicates()\n",
    "\n",
    "    df2_with_recency = df2_test2.merge(df_recency, on='ID_BUYER')\n",
    "\n",
    "    df2_with_recency_and_frquency = df2_with_recency.merge(frequency_df, on='ID_BUYER',how='left')\n",
    "\n",
    "    df2_with_recency_and_frquency['Frequnecy_comment_12M'].fillna(0, inplace=True)\n",
    "\n",
    "    return df2_with_recency_and_frquency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the three dataframes for the final model\n",
    "\n",
    "def merge_dataframes(df_transac, df_likes_final, df_comments_final):\n",
    "    # Merge the dataframes\n",
    "    df_merge = pd.merge(df_transac, df_likes_final, on='ID_BUYER', how='left')\n",
    "    df_merge = pd.merge(df_merge, df_comments_final, on='ID_BUYER', how='left')\n",
    "\n",
    "    # Export as csv\n",
    "\n",
    "    df_merge.to_csv(\"df_model_v4.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main call to the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transac = data_processing_mmao_transac(df_mmao, df_transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_likes_final = data_processing_likes(df_likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_final = data_preprocessing_comment(df_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataframes(df_transac, df_likes_final, df_comments_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
