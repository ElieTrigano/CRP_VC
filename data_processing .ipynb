{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# display all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document we will proceed to the import of the important of the raw datasets extracted from Vestiaire Collective's datalake. We will proceed data wrangling, data cleaning and data transformation to obtain a clean dataset ready to be used for the analysis. The final dataframe will be called \"df_model\" as it will be then used for the modelisation part."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMAO & TRANSACTION DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a part of the data\n",
    "\n",
    "df_mmao = pd.read_csv('Original_Data\\LIST_MMAO.csv', sep = '|')\n",
    "\n",
    "df_transaction = pd.read_csv('Original_Data\\LIST_TRANSACTION.csv', sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing_mmao_transac(df_mmao, df_transaction):\n",
    "    # Drop the columns with more than 15% null values\n",
    "    df = df_transaction.dropna(thresh=0.85*len(df_transaction), axis=1)\n",
    "\n",
    "    # Keeping only relevant features for our project\n",
    "    df_tri = df[['ID_PRODUCT',\n",
    "    'ID_ORDER',\n",
    "    'ID_CATEGORY',\n",
    "    'ID_SUBCATEGORY',\n",
    "    'ID_CONDITION',\n",
    "    'ID_BRAND',\n",
    "    'UNIVERSE',\n",
    "    'DEPOSIT_PRICE',\n",
    "    'PRICE_SOLD_GMV',\n",
    "    'NB_ITEMS',\n",
    "    'DISCOUNT_AMOUNT_GMV',\n",
    "    'DATE_SOLD',\n",
    "    'DATE_PUBLISHED',\n",
    "    'ID_PAYMENT_TYPE',\n",
    "    'ID_BUYER',\n",
    "    'RANK_BUYER',\n",
    "    'RANK_WITHIN_SEGMENT',\n",
    "    'ID_SEGMENT',\n",
    "    'LOVED',\n",
    "    'ORDER_VALUE_RANGE',\n",
    "    'BUYER_FEE_GMV',\n",
    "    'ORDER_MARKETING_CHANNEL',\n",
    "    'MMAO_PRICE_DROP',\n",
    "    'VOUCHER_REVENUE',\n",
    "    'BUYER_TYPE',\n",
    "    'FLAG_FRAUD',\n",
    "    'ID_GENDER',\n",
    "    'DATE_NEW_BUYER',\n",
    "    'DATE_CREATION',\n",
    "    'DATE_LAST_LOGIN',\n",
    "    'INACTIVE',\n",
    "    'NB_SOLD',\n",
    "    'VALUE_SOLD',\n",
    "    'NB_PUBLISHED',\n",
    "    'VALUE_PUBLISHED',\n",
    "    'NB_PURCHASED',\n",
    "    'VALUE_PURCHASED',\n",
    "    'NL_REACTIVITY_GROUP',\n",
    "    'ID_RFM_BUYER',\n",
    "    'RFM_BUYER',\n",
    "    'USER_SEGMENT',\n",
    "    'DATE_LAST_PURCHASE',\n",
    "    'DATE_FIRST_PURCHASE',\n",
    "    'BUYER_SEGMENT']]\n",
    "\n",
    "    # Creation of the two target variables \n",
    "    ##1. Is the buyer a repeater?\n",
    "    ### Convert buyer types to 0 and 1\n",
    "    df_tri['REPEATER'] = df_tri['BUYER_TYPE'].map({'New_Buyer': 0, 'Repeat': 1, 'Repeat_90D': 1})\n",
    "\n",
    "    ## 2. CLTV\n",
    "    ### Add the value purchased by the buyer and the value sold by the buyer\n",
    "    df_tri['CLTV'] = df_tri['VALUE_PURCHASED'] + df_tri['VALUE_SOLD']\n",
    "\n",
    "    # Drop the columns that are not needed anymore\n",
    "    df_transac_final = df_tri.drop(['BUYER_TYPE', 'VALUE_PURCHASED', 'VALUE_SOLD', 'VALUE_PUBLISHED'], axis=1)\n",
    "\n",
    "    # Create a feature with the number of offers grouped by each buyer\n",
    "    df_mmao['NB_OFFERS'] = df_mmao.groupby('ID_BUYER')['NB_TOTAL_OFFERS'].transform('sum')\n",
    "    df_mmao['AVG_OFFERS'] = df_mmao.groupby('ID_BUYER')['NB_TOTAL_OFFERS'].transform('mean')\n",
    "\n",
    "    # Drop the duplicates in mmao dataframe\n",
    "    df_mmao_final = df_mmao[['ID_BUYER', 'NB_OFFERS', 'AVG_OFFERS']].drop_duplicates()\n",
    "\n",
    "    # Merge the two dataframes\n",
    "\n",
    "    df_tr_mmao = pd.merge(df_transac_final, df_mmao_final, on='ID_BUYER', how='left')\n",
    "\n",
    "    # Fill null values due to the merge and drop the remaining values \n",
    "\n",
    "    df_tr_mmao['NB_OFFERS'].fillna(0, inplace=True)\n",
    "    df_tr_mmao['AVG_OFFERS'].fillna(0, inplace=True)\n",
    "\n",
    "    df_tr_mmao.dropna(inplace=True)\n",
    "\n",
    "    # Drop unrelevant features for the project \n",
    "    df_tr_mmao.drop(['ID_PRODUCT','ID_ORDER','UNIVERSE','LOVED','FLAG_FRAUD'], axis=1, inplace=True)\n",
    "\n",
    "    # Create feature number of days between date sold and date published, then drop the two columns\n",
    "\n",
    "    df_tr_mmao['DATE_SOLD'] = pd.to_datetime(df_tr_mmao['DATE_SOLD'])\n",
    "    df_tr_mmao['DATE_PUBLISHED'] = pd.to_datetime(df_tr_mmao['DATE_PUBLISHED'])\n",
    "    df_tr_mmao['NB_DAYS_ONLINE'] = (df_tr_mmao['DATE_SOLD'] - df_tr_mmao['DATE_PUBLISHED']).dt.days\n",
    "    df_tr_mmao.drop(['DATE_SOLD','DATE_PUBLISHED'], axis=1, inplace=True)\n",
    "\n",
    "    df_tr_mmao_test = df_tr_mmao.copy()\n",
    "\n",
    "    # Create aggregated features as we are grouping on ID_BUYER\n",
    "\n",
    "    ## Categorical or data that will be grouped by mode value\n",
    "    grouped = df_tr_mmao_test.groupby('ID_BUYER')\n",
    "    agg_mode = grouped.agg({'ID_CATEGORY': pd.Series.mode, 'ID_SUBCATEGORY': pd.Series.mode, 'ID_BRAND': pd.Series.mode,'ID_PAYMENT_TYPE': pd.Series.mode,'ORDER_MARKETING_CHANNEL': pd.Series.mode})\n",
    "    agg_mode= agg_mode.reset_index()\n",
    "    df_tr_mmao_test = df_tr_mmao_test.merge(agg_mode, on='ID_BUYER', how='left')\n",
    "    df_tr_mmao_test = df_tr_mmao_test.drop(['ID_CATEGORY_x', 'ID_SUBCATEGORY_x', 'ID_BRAND_x', 'ID_PAYMENT_TYPE_x', 'ORDER_MARKETING_CHANNEL_x'], axis=1)\n",
    "    df_tr_mmao_test = df_tr_mmao_test.rename(columns={'ID_CATEGORY_y': 'ID_CATEGORY','ID_SUBCATEGORY_y': 'ID_SUBCATEGORY','ID_BRAND_y': 'ID_BRAND','ID_PAYMENT_TYPE_y': 'ID_PAYMENT_TYPE', 'ORDER_MARKETING_CHANNEL_y': 'ORDER_MARKETING_CHANNEL'})\n",
    "\n",
    "    ## Numerical or data that will be grouped by mean value\n",
    "    agg_mean = df_tr_mmao_test.groupby('ID_BUYER').agg({'DEPOSIT_PRICE': 'mean', 'PRICE_SOLD_GMV': 'mean', 'NB_ITEMS': 'mean','DISCOUNT_AMOUNT_GMV': 'mean','BUYER_FEE_GMV': 'mean','MMAO_PRICE_DROP': 'mean', 'VOUCHER_REVENUE': 'mean', 'ID_CONDITION': 'mean', 'ID_GENDER': 'mean'})\n",
    "    agg_mean = agg_mean.reset_index()\n",
    "    df_tr_mmao_test = df_tr_mmao_test.merge(agg_mean, on='ID_BUYER', how='left')\n",
    "    df_tr_mmao_test = df_tr_mmao_test.drop(['DEPOSIT_PRICE_x', 'PRICE_SOLD_GMV_x', 'NB_ITEMS_x','DISCOUNT_AMOUNT_GMV_x','BUYER_FEE_GMV_x','MMAO_PRICE_DROP_x', 'VOUCHER_REVENUE_x','ID_CONDITION_x', 'ID_GENDER_x'], axis=1)\n",
    "    df_tr_mmao_test = df_tr_mmao_test.rename(columns={'DEPOSIT_PRICE_y': 'DEPOSIT_PRICE','PRICE_SOLD_GMV_y': 'PRICE_SOLD_GMV','NB_ITEMS_y': 'NB_ITEMS','DISCOUNT_AMOUNT_GMV_y': 'DISCOUNT_AMOUNT_GMV','BUYER_FEE_GMV_y': 'BUYER_FEE_GMV','MMAO_PRICE_DROP_y': 'MMAO_PRICE_DROP', 'VOUCHER_REVENUE_y': 'VOUCHER_REVENUE','ID_CONDITION_y': 'ID_CONDITION', 'ID_GENDER_y':'ID_GENDER' })\n",
    "\n",
    "    # Remap the gender values \n",
    "    df_tr_mmao_test.loc[df_tr_mmao_test['ID_GENDER'] == 0, 'ID_GENDER'] = 1\n",
    "    df_tr_mmao_test.loc[df_tr_mmao_test['ID_GENDER'] == 1, 'ID_GENDER'] = 0\n",
    "    df_tr_mmao_test.loc[df_tr_mmao_test['ID_GENDER'] == 2, 'ID_GENDER'] = 1\n",
    "    df_tr_mmao_test.loc[df_tr_mmao_test['ID_GENDER'] == 3, 'ID_GENDER'] = 2\n",
    "\n",
    "    df_final_v1 = df_tr_mmao_test.copy()\n",
    "\n",
    "    # groupby ID_BUYER and if in the column REPEATER there is a 1, change all other values to 1 as well\n",
    "\n",
    "    df_final_v1['REPEATER'] = df_final_v1.groupby('ID_BUYER')['REPEATER'].transform('max')\n",
    "\n",
    "    # create a dataframe with unique ID_BUYERS by grouping by ID_BUYER \n",
    "    df_final_v1 = df_final_v1.groupby('ID_BUYER').first().reset_index()\n",
    "\n",
    "    return df_final_v1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIKES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_likes = pd.read_csv('Original_Data\\LIST_LIKES.csv', sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing_likes(df_likes):\n",
    "\n",
    "    df = df_likes.dropna(thresh=0.85*len(df_likes), axis=1)\n",
    "\n",
    "    df2 = df[[\"ID_CLIENT\",\n",
    "            \"ID_BUYER\",\n",
    "            \"ID_LIKE\",\n",
    "            \"DATE_LIKED\",\n",
    "            \"DATE_PUBLISHED\",\n",
    "            \"UPDATEDAT\",\n",
    "            \"NBWISH\",\n",
    "            \"NB_CONSULTATION\",\n",
    "            \"MMAO_NB\",\n",
    "            \"LIKES\",\n",
    "            \"WISHES\",\n",
    "            \"LOVED\",\n",
    "            \"IS_ONLINE\",\n",
    "            \"IS_ACTIVE\",\n",
    "            \"IS_WITHDRAWN\",\n",
    "            \"IS_REJECTED\",\n",
    "            \"IS_RESERVED\",\n",
    "            \"STATUS_AFTER_7_DAYS\",\n",
    "            \"CURRENT_STATUS\",\n",
    "            \"ID_PRODUCT\",\n",
    "            \"ID_CATEGORY\",\n",
    "            \"ID_SUBCATEGORY\",\n",
    "            \"ID_BRAND\",\n",
    "            \"ID_UNIVERSE\",\n",
    "            \"ID_CONDITION\",\n",
    "            \"SEGMENT\",\n",
    "            \"ONED_SOLD_STATUS\",\n",
    "            \"TWOD_SOLD_STATUS\",\n",
    "            \"THREED_SOLD_STATUS\",\n",
    "            \"SEVEND_SOLD_STATUS\",\n",
    "            \"FIFTEEND_SOLD_STATUS\",\n",
    "            \"THIRTYD_SOLD_STATUS\",\n",
    "            \"NINETYD_SOLD_STATUS\",\n",
    "            \"ID_LAST_ACTION\",\n",
    "            \"IS_ITEM_WHITELISTED\",\n",
    "            \"IS_NEWIN_LIKED_IN_7DAYS\",\n",
    "            \"IS_NEWIN_WISHLISTED_IN_7DAYS\",\n",
    "            \"IS_NEWIN_MMAO_IN_7DAYS\",\n",
    "            \"IS_NEWIN_COMMENTED_IN_7DAYS\",\n",
    "            \"IS_NEWIN_ATC_IN_7DAYS\",\n",
    "            \"COMMISSION\"\n",
    "    ]]\n",
    "\n",
    "    df2 = df2.drop(['IS_ONLINE', 'IS_ACTIVE', 'IS_REJECTED','IS_RESERVED', 'IS_ITEM_WHITELISTED', 'ID_LAST_ACTION', 'NB_CONSULTATION',\n",
    "                    'ONED_SOLD_STATUS', 'TWOD_SOLD_STATUS', 'THREED_SOLD_STATUS', \n",
    "                    'SEVEND_SOLD_STATUS', 'FIFTEEND_SOLD_STATUS', 'THIRTYD_SOLD_STATUS',\n",
    "                    'NINETYD_SOLD_STATUS', 'IS_WITHDRAWN', 'IS_NEWIN_LIKED_IN_7DAYS',\n",
    "                    'IS_NEWIN_MMAO_IN_7DAYS', 'IS_NEWIN_WISHLISTED_IN_7DAYS', 'IS_NEWIN_COMMENTED_IN_7DAYS',\n",
    "                    'IS_NEWIN_ATC_IN_7DAYS', 'ID_UNIVERSE', 'LOVED', 'SEGMENT', \n",
    "                    'NBWISH', 'DATE_PUBLISHED', 'UPDATEDAT', \n",
    "                    'ID_BRAND', 'ID_CONDITION', 'ID_SUBCATEGORY', 'STATUS_AFTER_7_DAYS','CURRENT_STATUS','ID_CLIENT'], axis=1)\n",
    "\n",
    "    df2_test = df2.copy()\n",
    "\n",
    "    # Create features with the total number of likes, wishes and consultations grouped by each buyer\n",
    "    df2_test['Total_likes'] = df2_test.groupby('ID_BUYER')['LIKES'].transform('sum')\n",
    "    df2_test['Total_wishes'] = df2_test.groupby('ID_BUYER')['WISHES'].transform('sum')\n",
    "    df2_test['Total_MMAO_NB'] = df2_test.groupby('ID_BUYER')['MMAO_NB'].transform('sum')\n",
    "\n",
    "    # Create features with the unique number of commented products and categories grouped by each buyer\n",
    "    df2_test['NB_products_liked'] = df2_test.groupby('ID_BUYER')['ID_PRODUCT'].transform('nunique')\n",
    "    df2_test['NB_categories_liked'] = df2_test.groupby('ID_BUYER')['ID_CATEGORY'].transform('nunique')\n",
    "\n",
    "    # Create a feature with average commission grouped by each buyer\n",
    "    df2_test['Avg_commision'] = df2_test.groupby('ID_BUYER')['COMMISSION'].transform('mean')\n",
    "\n",
    "    df2_test['DATE_LIKED'] = pd.to_datetime(df2_test['DATE_LIKED'])\n",
    "\n",
    "    #Feature for the last comment made grouped by ID_BUYER\n",
    "    #recency is calculated based on difference of days from last comment ever made on platform\n",
    "    df_recency = df2_test.groupby(by='ID_BUYER',\n",
    "                            as_index=False)['DATE_LIKED'].max()\n",
    "    df_recency.columns = ['ID_BUYER', 'LastLikeDate']\n",
    "    df_recency['LastLikeDate'] = pd.to_datetime(df_recency['LastLikeDate'])\n",
    "    recent_date = df_recency['LastLikeDate'].max()\n",
    "    df_recency['Recency_liked'] = df_recency['LastLikeDate'].apply(\n",
    "        lambda x: (recent_date - x).days)\n",
    "    df_recency.head()\n",
    "\n",
    "\n",
    "    filtered_df = df2_test[~(df2_test['DATE_LIKED'] < '2022-01-01')]\n",
    "\n",
    "    frequency_df = filtered_df.groupby(\n",
    "        by=['ID_BUYER'], as_index=False)['DATE_LIKED'].count()\n",
    "    frequency_df.columns = ['ID_BUYER', 'Frequnecy_like_12M']\n",
    "    frequency_df.head()\n",
    "\n",
    "    df2_test2 = df2_test[['ID_BUYER', 'Total_likes', 'Total_wishes', 'Total_MMAO_NB', 'NB_products_liked', 'NB_categories_liked', 'Avg_commision']].drop_duplicates()\n",
    "\n",
    "    df2_with_recency = df2_test2.merge(df_recency, on='ID_BUYER')\n",
    "\n",
    "    df2_with_recency_and_frquency = df2_with_recency.merge(frequency_df, on='ID_BUYER',how='left')\n",
    "\n",
    "    df2_with_recency_and_frquency['Frequnecy_like_12M'].fillna(0, inplace=True)\n",
    "\n",
    "    return df2_with_recency_and_frquency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMMENTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment = pd.read_csv('Original_Data\\LIST_COMMENT.csv', sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing_comment(df_comment):\n",
    "    # Drop the columns with more than 20% null values\n",
    "    df = df_comment.dropna(thresh=0.80*len(df_comment), axis=1)\n",
    "\n",
    "\n",
    "    df2 = df[[\"ID_CLIENT\",\n",
    "            \"ID_BUYER\",\n",
    "            \"ID_COMMENT\",\n",
    "            \"DATE_COMMENT\",\n",
    "            \"DATE_PUBLISHED\",\n",
    "            \"UPDATEDAT\",\n",
    "            \"NB_LIKES\",\n",
    "            \"NBWISH\",\n",
    "            \"NB_CONSULTATION\",\n",
    "            \"LIKES\",\n",
    "            \"WISHES\",\n",
    "            \"LOVED\",\n",
    "            \"IS_ONLINE\",\n",
    "            \"IS_ACTIVE\",\n",
    "            \"IS_WITHDRAWN\",\n",
    "            \"IS_REJECTED\",\n",
    "            \"IS_RESERVED\",\n",
    "            \"STATUS_AFTER_7_DAYS\",\n",
    "            \"CURRENT_STATUS\",\n",
    "            \"ACCEPTED_BY\",\n",
    "            \"CURATOR_TYPE\",\n",
    "            \"ID_PRODUCT\",\n",
    "            \"ID_CATEGORY\",\n",
    "            \"ID_SUBCATEGORY\",\n",
    "            \"ID_BRAND\",\n",
    "            \"UNIVERSE\",\n",
    "            \"ID_CONDITION\",\n",
    "            \"SEGMENT\",\n",
    "            \"ONED_SOLD_STATUS\",\n",
    "            \"TWOD_SOLD_STATUS\",\n",
    "            \"THREED_SOLD_STATUS\",\n",
    "            \"SEVEND_SOLD_STATUS\",\n",
    "            \"FIFTEEND_SOLD_STATUS\",\n",
    "            \"THIRTYD_SOLD_STATUS\",\n",
    "            \"NINETYD_SOLD_STATUS\",\n",
    "            \"ID_LAST_ACTION\",\n",
    "            \"IS_ITEM_WHITELISTED\",\n",
    "            \"IS_NEWIN_LIKED_IN_7DAYS\",\n",
    "            \"IS_NEWIN_WISHLISTED_IN_7DAYS\",\n",
    "            \"IS_NEWIN_MMAO_IN_7DAYS\",\n",
    "            \"IS_NEWIN_COMMENTED_IN_7DAYS\",\n",
    "            \"IS_NEWIN_ATC_IN_7DAYS\",\n",
    "            \"COMMISSION\"\n",
    "    ]]\n",
    "\n",
    "    df2 = df2.drop(['CURATOR_TYPE', 'IS_ONLINE', 'IS_ACTIVE', 'IS_REJECTED','IS_RESERVED', 'IS_ITEM_WHITELISTED','ACCEPTED_BY', 'ID_LAST_ACTION', 'NB_CONSULTATION'], axis=1)\n",
    "    df2 = df2.drop(['ONED_SOLD_STATUS', 'TWOD_SOLD_STATUS', 'THREED_SOLD_STATUS', \n",
    "                'SEVEND_SOLD_STATUS', 'FIFTEEND_SOLD_STATUS', 'THIRTYD_SOLD_STATUS',\n",
    "                'NINETYD_SOLD_STATUS', 'IS_WITHDRAWN', 'IS_NEWIN_LIKED_IN_7DAYS',\n",
    "                'IS_NEWIN_MMAO_IN_7DAYS', 'IS_NEWIN_WISHLISTED_IN_7DAYS', 'IS_NEWIN_COMMENTED_IN_7DAYS',\n",
    "                'IS_NEWIN_ATC_IN_7DAYS', 'UNIVERSE', 'LOVED', 'SEGMENT', \n",
    "                'LIKES', 'WISHES', 'DATE_PUBLISHED', 'UPDATEDAT', \n",
    "                'ID_BRAND', 'ID_CONDITION', 'ID_SUBCATEGORY', 'STATUS_AFTER_7_DAYS','CURRENT_STATUS'], axis=1)\n",
    "    \n",
    "    df2 = df2.drop(['ID_CLIENT'], axis=1)\n",
    "\n",
    "\n",
    "    df2_test = df2.copy()\n",
    "\n",
    "    # Create features with the total number of likes, wishes and consultations grouped by each buyer\n",
    "\n",
    "    df2_test['Total_nb_likes'] = df2_test.groupby('ID_BUYER')['NB_LIKES'].transform('sum')\n",
    "    df2_test['Total_nb_wish'] = df2_test.groupby('ID_BUYER')['NBWISH'].transform('sum')\n",
    "\n",
    "    # Create features with the unique number of commented products and categories grouped by each buyer\n",
    "    df2_test['NB_products_commented'] = df2_test.groupby('ID_BUYER')['ID_PRODUCT'].transform('nunique')\n",
    "    df2_test['NB_categories_commented'] = df2_test.groupby('ID_BUYER')['ID_CATEGORY'].transform('nunique')\n",
    "\n",
    "\n",
    "\n",
    "    # Create a feature with average commission grouped by each buyer\n",
    "    df2_test['Avg_commision'] = df2_test.groupby('ID_BUYER')['COMMISSION'].transform('mean')\n",
    "\n",
    "    df2_test['DATE_COMMENT'] = pd.to_datetime(df2_test['DATE_COMMENT'])\n",
    "\n",
    "    #Feature for the last comment made grouped by ID_BUYER\n",
    "    #recency is calculated based on difference of days from last comment ever made on platform\n",
    "    df_recency = df2_test.groupby(by='ID_BUYER',\n",
    "                            as_index=False)['DATE_COMMENT'].max()\n",
    "    df_recency.columns = ['ID_BUYER', 'LastCommentDate']\n",
    "    df_recency['LastCommentDate'] = pd.to_datetime(df_recency['LastCommentDate'])\n",
    "    recent_date = df_recency['LastCommentDate'].max()\n",
    "    df_recency['Recency_comment'] = df_recency['LastCommentDate'].apply(\n",
    "        lambda x: (recent_date - x).days)\n",
    "\n",
    "    filtered_df = df2_test[~(df2_test['DATE_COMMENT'] < '2022-01-01')]\n",
    "\n",
    "\n",
    "    frequency_df = filtered_df.groupby(\n",
    "        by=['ID_BUYER'], as_index=False)['DATE_COMMENT'].count()\n",
    "    frequency_df.columns = ['ID_BUYER', 'Frequnecy_comment_12M']\n",
    "\n",
    "    df2_test2 = df2_test[['ID_BUYER', 'Total_nb_likes', 'Total_nb_wish', 'NB_products_commented', 'NB_categories_commented', 'Avg_commision']].drop_duplicates()\n",
    "\n",
    "    df2_with_recency = df2_test2.merge(df_recency, on='ID_BUYER')\n",
    "\n",
    "    df2_with_recency_and_frquency = df2_with_recency.merge(frequency_df, on='ID_BUYER',how='left')\n",
    "\n",
    "    df2_with_recency_and_frquency['Frequnecy_comment_12M'].fillna(0, inplace=True)\n",
    "\n",
    "    df2_with_recency_and_frquency.to_csv('df_comment_final.csv', index=False)\n",
    "\n",
    "    return df2_with_recency_and_frquency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the three dataframes for the final model\n",
    "\n",
    "def merge_dataframes(df_transac, df_likes_final, df_comments_final):\n",
    "    # Merge the dataframes\n",
    "    df_merge = pd.merge(df_transac, df_likes_final, on='ID_BUYER', how='left')\n",
    "    df_merge = pd.merge(df_merge, df_comments_final, on='ID_BUYER', how='left')\n",
    "\n",
    "    # Export as csv\n",
    "\n",
    "    df_merge.to_csv(\"df_model_v5.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main call to the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transac = data_processing_mmao_transac(df_mmao, df_transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_likes_final = data_processing_likes(df_likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_BUYER</th>\n",
       "      <th>Total_likes</th>\n",
       "      <th>Total_wishes</th>\n",
       "      <th>Total_MMAO_NB</th>\n",
       "      <th>NB_products_liked</th>\n",
       "      <th>NB_categories_liked</th>\n",
       "      <th>Avg_commision</th>\n",
       "      <th>LastLikeDate</th>\n",
       "      <th>Recency_liked</th>\n",
       "      <th>Frequnecy_like_12M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13631180</td>\n",
       "      <td>21948.0</td>\n",
       "      <td>1519.0</td>\n",
       "      <td>5575.0</td>\n",
       "      <td>478</td>\n",
       "      <td>5</td>\n",
       "      <td>57.260050</td>\n",
       "      <td>2022-11-24 00:26:41</td>\n",
       "      <td>50</td>\n",
       "      <td>464.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14475477</td>\n",
       "      <td>76477.0</td>\n",
       "      <td>4139.0</td>\n",
       "      <td>14664.0</td>\n",
       "      <td>1026</td>\n",
       "      <td>5</td>\n",
       "      <td>160.538168</td>\n",
       "      <td>2023-01-13 10:32:43</td>\n",
       "      <td>0</td>\n",
       "      <td>1021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8290075</td>\n",
       "      <td>3072.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>753.0</td>\n",
       "      <td>71</td>\n",
       "      <td>5</td>\n",
       "      <td>23.171711</td>\n",
       "      <td>2023-01-10 10:31:54</td>\n",
       "      <td>3</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15785692</td>\n",
       "      <td>73866.0</td>\n",
       "      <td>4105.0</td>\n",
       "      <td>11126.0</td>\n",
       "      <td>1355</td>\n",
       "      <td>6</td>\n",
       "      <td>61.494906</td>\n",
       "      <td>2023-01-12 22:22:55</td>\n",
       "      <td>0</td>\n",
       "      <td>1411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15679827</td>\n",
       "      <td>34934.0</td>\n",
       "      <td>2741.0</td>\n",
       "      <td>6293.0</td>\n",
       "      <td>1333</td>\n",
       "      <td>7</td>\n",
       "      <td>18.974401</td>\n",
       "      <td>2023-01-09 19:07:39</td>\n",
       "      <td>3</td>\n",
       "      <td>1343.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_BUYER  Total_likes  Total_wishes  Total_MMAO_NB  NB_products_liked  \\\n",
       "0  13631180      21948.0        1519.0         5575.0                478   \n",
       "1  14475477      76477.0        4139.0        14664.0               1026   \n",
       "2   8290075       3072.0         181.0          753.0                 71   \n",
       "3  15785692      73866.0        4105.0        11126.0               1355   \n",
       "4  15679827      34934.0        2741.0         6293.0               1333   \n",
       "\n",
       "   NB_categories_liked  Avg_commision        LastLikeDate  Recency_liked  \\\n",
       "0                    5      57.260050 2022-11-24 00:26:41             50   \n",
       "1                    5     160.538168 2023-01-13 10:32:43              0   \n",
       "2                    5      23.171711 2023-01-10 10:31:54              3   \n",
       "3                    6      61.494906 2023-01-12 22:22:55              0   \n",
       "4                    7      18.974401 2023-01-09 19:07:39              3   \n",
       "\n",
       "   Frequnecy_like_12M  \n",
       "0               464.0  \n",
       "1              1021.0  \n",
       "2                68.0  \n",
       "3              1411.0  \n",
       "4              1343.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_likes_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_final = data_preprocessing_comment(df_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_BUYER</th>\n",
       "      <th>Total_nb_likes</th>\n",
       "      <th>Total_nb_wish</th>\n",
       "      <th>NB_products_commented</th>\n",
       "      <th>NB_categories_commented</th>\n",
       "      <th>Avg_commision</th>\n",
       "      <th>LastCommentDate</th>\n",
       "      <th>Recency_comment</th>\n",
       "      <th>Frequnecy_comment_12M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4242578</td>\n",
       "      <td>1005</td>\n",
       "      <td>36833.0</td>\n",
       "      <td>77</td>\n",
       "      <td>5</td>\n",
       "      <td>24.582079</td>\n",
       "      <td>2023-01-02 23:32:11</td>\n",
       "      <td>10</td>\n",
       "      <td>12821.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10388157</td>\n",
       "      <td>153</td>\n",
       "      <td>21441.0</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>47.049400</td>\n",
       "      <td>2022-12-07 21:16:49</td>\n",
       "      <td>36</td>\n",
       "      <td>6014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>715817</td>\n",
       "      <td>620</td>\n",
       "      <td>83248.0</td>\n",
       "      <td>69</td>\n",
       "      <td>3</td>\n",
       "      <td>87.581265</td>\n",
       "      <td>2022-12-23 09:38:39</td>\n",
       "      <td>20</td>\n",
       "      <td>3531.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15964728</td>\n",
       "      <td>154</td>\n",
       "      <td>462.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25.250794</td>\n",
       "      <td>2022-03-09 07:52:35</td>\n",
       "      <td>309</td>\n",
       "      <td>462.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9141195</td>\n",
       "      <td>306</td>\n",
       "      <td>92755.0</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>108.344187</td>\n",
       "      <td>2022-09-12 14:08:50</td>\n",
       "      <td>122</td>\n",
       "      <td>10447.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_BUYER  Total_nb_likes  Total_nb_wish  NB_products_commented  \\\n",
       "0   4242578            1005        36833.0                     77   \n",
       "1  10388157             153        21441.0                     23   \n",
       "2    715817             620        83248.0                     69   \n",
       "3  15964728             154          462.0                      2   \n",
       "4   9141195             306        92755.0                     13   \n",
       "\n",
       "   NB_categories_commented  Avg_commision     LastCommentDate  \\\n",
       "0                        5      24.582079 2023-01-02 23:32:11   \n",
       "1                        5      47.049400 2022-12-07 21:16:49   \n",
       "2                        3      87.581265 2022-12-23 09:38:39   \n",
       "3                        1      25.250794 2022-03-09 07:52:35   \n",
       "4                        3     108.344187 2022-09-12 14:08:50   \n",
       "\n",
       "   Recency_comment  Frequnecy_comment_12M  \n",
       "0               10                12821.0  \n",
       "1               36                 6014.0  \n",
       "2               20                 3531.0  \n",
       "3              309                  462.0  \n",
       "4              122                10447.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataframes(df_transac, df_likes_final, df_comments_final)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing to see if we have the final output desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = pd.read_csv('df_model_v5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID_BUYER                       0\n",
       "RANK_BUYER                     0\n",
       "RANK_WITHIN_SEGMENT            0\n",
       "ID_SEGMENT                     0\n",
       "ORDER_VALUE_RANGE              0\n",
       "DATE_NEW_BUYER                 0\n",
       "DATE_CREATION                  0\n",
       "DATE_LAST_LOGIN                0\n",
       "INACTIVE                       0\n",
       "NB_SOLD                        0\n",
       "NB_PUBLISHED                   0\n",
       "NB_PURCHASED                   0\n",
       "NL_REACTIVITY_GROUP            0\n",
       "ID_RFM_BUYER                   0\n",
       "RFM_BUYER                      0\n",
       "USER_SEGMENT                   0\n",
       "DATE_LAST_PURCHASE             0\n",
       "DATE_FIRST_PURCHASE            0\n",
       "BUYER_SEGMENT                  0\n",
       "REPEATER                       0\n",
       "CLTV                           0\n",
       "NB_OFFERS                      0\n",
       "AVG_OFFERS                     0\n",
       "NB_DAYS_ONLINE                 0\n",
       "ID_CATEGORY                    0\n",
       "ID_SUBCATEGORY                 0\n",
       "ID_BRAND                       0\n",
       "ID_PAYMENT_TYPE                0\n",
       "ORDER_MARKETING_CHANNEL        0\n",
       "DEPOSIT_PRICE                  0\n",
       "PRICE_SOLD_GMV                 0\n",
       "NB_ITEMS                       0\n",
       "DISCOUNT_AMOUNT_GMV            0\n",
       "BUYER_FEE_GMV                  0\n",
       "MMAO_PRICE_DROP                0\n",
       "VOUCHER_REVENUE                0\n",
       "ID_CONDITION                   0\n",
       "ID_GENDER                      0\n",
       "Total_likes                10889\n",
       "Total_wishes               10889\n",
       "Total_MMAO_NB              10889\n",
       "NB_products_liked          10889\n",
       "NB_categories_liked        10889\n",
       "Avg_commision_x            10889\n",
       "LastLikeDate               10889\n",
       "Recency_liked              10889\n",
       "Frequnecy_like_12M         10889\n",
       "Total_nb_likes             39220\n",
       "Total_nb_wish              39220\n",
       "NB_products_commented      39220\n",
       "NB_categories_commented    39220\n",
       "Avg_commision_y            39220\n",
       "LastCommentDate            39220\n",
       "Recency_comment            39220\n",
       "Frequnecy_comment_12M      39220\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.isnull().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
