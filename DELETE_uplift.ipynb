{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Uplift Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this model, please import the dataframe from the two first models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import shap\n",
    "\n",
    "import time\n",
    "import pickle as pkl\n",
    "from scipy import stats\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "# for classification\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score,multilabel_confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# for regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a treatment column to the dataframe\n",
    "# Randomly assign a 0 or 1 to each row. 1 for treatment, 0 for control\n",
    "df['treatment'] = np.random.randint(2, size=len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    26367\n",
       "1    26067\n",
       "Name: treatment, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.treatment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REPEATER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>treatment</th>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REPEATER</th>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           REPEATER\n",
       "treatment      0.29\n",
       "REPEATER     100.00"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the treatment's correlation to repeat purchases:\n",
    "def correlation_treatment(df:pd.DataFrame):\n",
    "    \"\"\"Function to calculate the treatment's correlation\n",
    "    \"\"\"\n",
    "    correlation = df[['treatment','REPEATER']].corr(method ='pearson') \n",
    "    return(pd.DataFrame(round(correlation.loc['REPEATER'] * 100,2)))\n",
    "\n",
    "correlation_treatment(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declare_target_class(df:pd.DataFrame):\n",
    "    \"\"\"Function for declare the target class\n",
    "    \"\"\"\n",
    "    # Control Non Repeaters (CN) : 0\n",
    "    df['target_class'] = 0 \n",
    "    # Control Repeaters (CR) : 1\n",
    "    df.loc[(df.treatment == 0) & (df.REPEATER == 0),'target_class'] = 1 \n",
    "    # Treatment Non Repeaters (TN) : 2\n",
    "    df.loc[(df.treatment == 1) & (df.REPEATER == 1),'target_class'] = 2 \n",
    "    # Treatment Repeaters (TR) : 3\n",
    "    df.loc[(df.treatment == 1) & (df.REPEATER == 0),'target_class'] = 3 \n",
    "    return df\n",
    "\n",
    "df = declare_target_class(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting dataset 1 ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:ORDER_VALUE_RANGE: object, DATE_NEW_BUYER: object, DATE_CREATION: object, DATE_LAST_LOGIN: object, NL_REACTIVITY_GROUP: object, RFM_BUYER: object, USER_SEGMENT: object, DATE_LAST_PURCHASE: object, DATE_FIRST_PURCHASE: object, BUYER_SEGMENT: object, ID_CATEGORY: object, ID_SUBCATEGORY: object, ID_BRAND: object, ID_PAYMENT_TYPE: object, ORDER_MARKETING_CHANNEL: object, LastLikeDate: object, LastCommentDate: object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5848\\32190031.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;31m# Machine Learning Modelling Process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Predicting dataset 1 ...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m \u001b[0mprediction_results_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5848\\32190031.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(df_model)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m     71\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     prediction_results = machine_learning(X_train,\n\u001b[0m\u001b[0;32m     73\u001b[0m                                           \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m                                           \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5848\\32190031.py\u001b[0m in \u001b[0;36mmachine_learning\u001b[1;34m(X_train, X_test, y_train, y_test, z_train, z_test)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# train the ETP model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mmodel_tp\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'treatment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[1;31m# prediction Process for ETP model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mprediction_tp\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1495\u001b[0m                 \u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1496\u001b[0m             )\n\u001b[1;32m-> 1497\u001b[1;33m             train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[0m\u001b[0;32m   1498\u001b[0m                 \u001b[0mmissing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36m_wrap_evaluation_matrices\u001b[1;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[0;32m    446\u001b[0m     \"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\n\u001b[0;32m    447\u001b[0m     way.\"\"\"\n\u001b[1;32m--> 448\u001b[1;33m     train_dmatrix = create_dmatrix(\n\u001b[0m\u001b[0;32m    449\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36m_create_dmatrix\u001b[1;34m(self, ref, **kwargs)\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_evaluation_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTrainingCallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEvalsLog\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical)\u001b[0m\n\u001b[0;32m    741\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 743\u001b[1;33m         handle, feature_names, feature_types = dispatch_data_backend(\n\u001b[0m\u001b[0;32m    744\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m             \u001b[0mmissing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\xgboost\\data.py\u001b[0m in \u001b[0;36mdispatch_data_backend\u001b[1;34m(data, missing, threads, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[0;32m    955\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_from_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_pandas_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m         return _from_pandas_df(data, enable_categorical, missing, threads,\n\u001b[0m\u001b[0;32m    958\u001b[0m                                feature_names, feature_types)\n\u001b[0;32m    959\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_pandas_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\xgboost\\data.py\u001b[0m in \u001b[0;36m_from_pandas_df\u001b[1;34m(data, enable_categorical, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[0mfeature_types\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mFeatureTypes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m ) -> DispatchedDataBackendReturnType:\n\u001b[1;32m--> 404\u001b[1;33m     data, feature_names, feature_types = _transform_pandas_df(\n\u001b[0m\u001b[0;32m    405\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menable_categorical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m     )\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\xgboost\\data.py\u001b[0m in \u001b[0;36m_transform_pandas_df\u001b[1;34m(data, enable_categorical, feature_names, feature_types, meta, meta_type)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m     ):\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[0m_invalid_dataframe_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     feature_names, feature_types = _pandas_feature_info(\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\xgboost\\data.py\u001b[0m in \u001b[0;36m_invalid_dataframe_dtype\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"DataFrame.dtypes for data must be int, float, bool or category.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\"\"{type_err} {_ENABLE_CAT_ERR} {err}\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:ORDER_VALUE_RANGE: object, DATE_NEW_BUYER: object, DATE_CREATION: object, DATE_LAST_LOGIN: object, NL_REACTIVITY_GROUP: object, RFM_BUYER: object, USER_SEGMENT: object, DATE_LAST_PURCHASE: object, DATE_FIRST_PURCHASE: object, BUYER_SEGMENT: object, ID_CATEGORY: object, ID_SUBCATEGORY: object, ID_BRAND: object, ID_PAYMENT_TYPE: object, ORDER_MARKETING_CHANNEL: object, LastLikeDate: object, LastCommentDate: object"
     ]
    }
   ],
   "source": [
    "def split_data(df_model:pd.DataFrame):\n",
    "    \"\"\"Split data into training data and testing data\n",
    "    \"\"\"\n",
    "    X = df_model.drop(['REPEATER', 'CLTV','target_class'],axis=1)\n",
    "    y = df_model.REPEATER\n",
    "    z = df_model.target_class\n",
    "    X_train, X_test, \\\n",
    "    y_train, y_test, \\\n",
    "    z_train, z_test = train_test_split(X,\n",
    "                                       y,\n",
    "                                       z,\n",
    "                                       test_size=0.3,\n",
    "                                       random_state=42,\n",
    "                                       stratify=df_model['treatment'])\n",
    "    return X_train,X_test, y_train, y_test, z_train, z_test\n",
    "\n",
    "\n",
    "def machine_learning(X_train:pd.DataFrame,\n",
    "                     X_test:pd.DataFrame,\n",
    "                     y_train:pd.DataFrame,\n",
    "                     y_test:pd.DataFrame,\n",
    "                     z_train:pd.DataFrame,\n",
    "                     z_test:pd.DataFrame):\n",
    "    \"\"\"Machine learning process consists of \n",
    "    data training, and data testing process (i.e. prediction) with XGBoost (XGB) Algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    # prepare a new DataFrame\n",
    "    prediction_results = pd.DataFrame(X_test).copy()\n",
    "    \n",
    "    \n",
    "    # train the ETP model\n",
    "    model_tp \\\n",
    "    = xgb.XGBClassifier().fit(X_train.drop('treatment', axis=1), y_train)  \n",
    "    # prediction Process for ETP model \n",
    "    prediction_tp \\\n",
    "    = model_tp.predict(X_test.drop('treatment',axis=1))\n",
    "    probability__tp \\\n",
    "    = model_tp.predict_proba(X_test.drop('treatment', axis=1))\n",
    "    prediction_results['prediction_REPEATER'] = prediction_tp\n",
    "    prediction_results['proba_REPEATER'] = probability__tp[:,1]\n",
    "    \n",
    "    \n",
    "    # train the ETU model\n",
    "    model_etu \\\n",
    "    = xgb.XGBClassifier().fit(X_train.drop('treatment', axis=1), z_train)\n",
    "    # prediction Process for ETU model \n",
    "    prediction_etu \\\n",
    "    = model_etu.predict(X_test.drop('treatment', axis=1))\n",
    "    probability__etu \\\n",
    "    = model_etu.predict_proba(X_test.drop('treatment', axis=1))\n",
    "    prediction_results['prediction_target_class'] = prediction_etu\n",
    "    prediction_results['proba_CN'] = probability__etu[:,0] \n",
    "    prediction_results['proba_CR'] = probability__etu[:,1] \n",
    "    prediction_results['proba_TN'] = probability__etu[:,2] \n",
    "    prediction_results['proba_TR'] = probability__etu[:,3]\n",
    "    prediction_results['score_etu'] = prediction_results.eval('\\\n",
    "    proba_CN/(proba_CN+proba_CR) \\\n",
    "    + proba_TR/(proba_TN+proba_TR) \\\n",
    "    - proba_TN/(proba_TN+proba_TR) \\\n",
    "    - proba_CR/(proba_CN+proba_CR)')  \n",
    "    # add the REPEATER and target class into dataframe as validation data\n",
    "    prediction_results['REPEATER'] = y_test\n",
    "    prediction_results['target_class'] = z_test\n",
    "    return prediction_results\n",
    "\n",
    "\n",
    "def predict(df_model:pd.DataFrame):\n",
    "    \"\"\"Combining data split and machine learning process with XGB\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test, z_train, z_test = split_data(df_model)\n",
    "    prediction_results = machine_learning(X_train,\n",
    "                                          X_test,\n",
    "                                          y_train,\n",
    "                                          y_test,\n",
    "                                          z_train,\n",
    "                                          z_test)\n",
    "    print(\"✔️Prediction succeeded\")\n",
    "    return prediction_results\n",
    "\n",
    "# Machine Learning Modelling Process\n",
    "print(\"Predicting dataset 1 ...\")\n",
    "prediction_results_1 = predict(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating predictive performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm_evaluation(df:pd.DataFrame):\n",
    "    \"\"\"Confusion matrix evaluation\n",
    "    \"\"\"  \n",
    "    print(\"===================================\")\n",
    "    print(\"1. ETP's confusion matrix result:\")\n",
    "    confusion_etp = confusion_matrix(df['REPEATER'], df['prediction_REPEATER'])\n",
    "    df_confusion_etp = pd.DataFrame(confusion_etp, columns = ['Predicted True','Predicted False'], index = ['Actual True','Actual False'])\n",
    "    print(df_confusion_etp)\n",
    "    \n",
    "    print(\"-----------------------------------\")\n",
    "    \n",
    "    print(\"2. ETU's confusion matrix result:\")   \n",
    "    confusion_etu = multilabel_confusion_matrix(df['target_class'], df['prediction_target_class'])\n",
    "    print(\"a. CN's confusion matrix:\")  \n",
    "    df_cn = pd.DataFrame(confusion_etu[0], columns = ['Predicted True','Predicted False'], index = ['Actual True','Actual False'])\n",
    "    print(df_cn)\n",
    "    print(\"b. CR's confusion matrix:\") \n",
    "    df_cr = pd.DataFrame(confusion_etu[1], columns = ['Predicted True','Predicted False'], index = ['Actual True','Actual False'])\n",
    "    print(df_cr) \n",
    "    print(\"c. TN's confusion matrix:\")\n",
    "    df_tn = pd.DataFrame(confusion_etu[2], columns = ['Predicted True','Predicted False'], index = ['Actual True','Actual False'])\n",
    "    print(df_tn) \n",
    "    print(\"d. TR's confusion matrix:\") \n",
    "    df_tr = pd.DataFrame(confusion_etu[3], columns = ['Predicted True','Predicted False'], index = ['Actual True','Actual False'])\n",
    "    print(df_tr)\n",
    "    \n",
    "    print(\"===================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Evaluation\n",
    "print(\"✔️Dataset 1\")\n",
    "cm_evaluation(prediction_results_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_evaluation(df:pd.DataFrame):\n",
    "    \"\"\"Accuracy evaluation\n",
    "    \"\"\"\n",
    "    akurasi_cp = accuracy_score(df['REPEATER'],\n",
    "                                df['prediction_REPEATER'])\n",
    "    print('✔️ETP model accuracy: %.2f%%' % (akurasi_cp * 100.0))\n",
    "    \n",
    "    \n",
    "    akurasi_uplift = accuracy_score(df['target_class'],\n",
    "                                    df['prediction_target_class'])\n",
    "    print('✔️ETU model accuracy: %.2f%%' % (akurasi_uplift * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Evaluation Process.\n",
    "print(\"Dataset 1\")\n",
    "accuracy_evaluation(prediction_results_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the prescriptive performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_data(df:pd.DataFrame):\n",
    "    \"\"\"Function to sort data\n",
    "    \"\"\"\n",
    "    # Set up new DataFrames for ETP model and ETU model\n",
    "    df_c = pd.DataFrame({'n':[], 'target_class':[]})\n",
    "    df_u = df_c.copy()\n",
    "    df_c['target_class'] = df['target_class']\n",
    "    df_u['target_class'] = df['target_class']\n",
    "    \n",
    "    \n",
    "    # Add quantiles\n",
    "    df_c['n'] = df.proba_REPEATER.rank(pct=True, ascending=False)\n",
    "    df_u['n'] = df.score_etu.rank(pct=True, ascending=False)\n",
    "    df_c['score'] = df['proba_REPEATER']\n",
    "    df_u['score'] = df['score_etu']\n",
    "    \n",
    "    \n",
    "    # Ranking the data by deciles\n",
    "    df_c = df_c.sort_values(by='n').reset_index(drop=True)\n",
    "    df_u = df_u.sort_values(by='n').reset_index(drop=True)\n",
    "    df_c['model'], df_u['model'] = 'CP', 'Uplift'\n",
    "    return df_c, df_u\n",
    "\n",
    "\n",
    "def calculating_qini(df:pd.DataFrame):\n",
    "    \"\"\"Function to measure the Qini value\n",
    "    \"\"\"\n",
    "    # Calculate the C, T, CR, and TR\n",
    "    C, T = sum(df['target_class'] <= 1), sum(df['target_class'] >= 2)\n",
    "    df['cr'] = 0\n",
    "    df['tr'] = 0\n",
    "    df.loc[df.target_class  == 1,'cr'] = 1\n",
    "    df.loc[df.target_class  == 3,'tr'] = 1\n",
    "    df['cr/c'] = df.cr.cumsum() / C\n",
    "    df['tr/t'] = df.tr.cumsum() / T\n",
    "    \n",
    "\n",
    "    # Calculate & add the qini value into the Dataframe\n",
    "    df['uplift'] = df['tr/t'] - df['cr/c']\n",
    "    df['random'] = df['n'] * df['uplift'].iloc[-1]\n",
    "    qini_coef= df['uplift'].sum(skipna = True) - df['random'].sum(skipna = True)\n",
    "\n",
    "    # Print the Qini coefficient\n",
    "    print('✔️Qini coefficient = {} {}'.format(round(qini_coef, 2), '%'))\n",
    "    \n",
    "    # Add q0 into the Dataframe\n",
    "    q0 = pd.DataFrame({'n':0, 'uplift':0, 'target_class': None}, index =[0])\n",
    "    qini = pd.concat([q0, df]).reset_index(drop = True)\n",
    "    return qini\n",
    "\n",
    "def merging_data(df_c:pd.DataFrame, df_u:pd.DataFrame):\n",
    "    \"\"\"Function to add the 'Model' column and merge the dataframe into one\n",
    "    \"\"\"\n",
    "    df_u['model'] = 'ETU'\n",
    "    df_c['model'] = 'ETP'\n",
    "    df = pd.concat([df_u, df_c]).sort_values(by='n').reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_qini(df:pd.DataFrame):\n",
    "    \"\"\"Function to plot the qini curve\n",
    "    \"\"\"\n",
    "    print('\\nPlotting the qini curve...')\n",
    "    \n",
    "    # Define the data that will be plotted\n",
    "    order = ['ETU','ETP']\n",
    "    ax = sns.lineplot(x='n', y=df.uplift, hue='model', data=df,\n",
    "                      style='model', palette=['red','deepskyblue'],\n",
    "                      style_order=order, hue_order = order)\n",
    "    \n",
    "    # Additional plot display settings\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    plt.xlabel('Proportion targeted',fontsize=30)\n",
    "    plt.ylabel('Uplift',fontsize=30)\n",
    "    plt.subplots_adjust(right=1)\n",
    "    plt.subplots_adjust(top=1)\n",
    "    plt.legend(fontsize=30)\n",
    "    ax.tick_params(labelsize=24)\n",
    "    ax.legend(handles=handles[1:], labels=labels[1:])\n",
    "    ax.plot([0,1], [0,df.loc[len(df) - 1,'uplift']],'--', color='grey')\n",
    "    print('✔️Successfully plot the qini curve')\n",
    "    return ax\n",
    "\n",
    "def evaluation_qini(prediction_results:pd.DataFrame):\n",
    "    \"\"\"Function to combine all qini evaluation processes\n",
    "    \"\"\"\n",
    "    df_c, df_u = sorting_data(prediction_results)\n",
    "    print('ETP model (previous model):')\n",
    "    qini_c = calculating_qini(df_c)\n",
    "    print('\\nETU model (our proposed model):')\n",
    "    qini_u = calculating_qini(df_u)\n",
    "    qini = merging_data(qini_c, qini_u)\n",
    "    ax = plot_qini(qini)\n",
    "    return ax, qini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qini evaluation results for DataSet 1 with negative treatment correlation\n",
    "ax, qini_1 = evaluation_qini(prediction_results_1)\n",
    "plt.title('Qini Curve - Dataset 1',fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The process to inverse treatment's parameter\n",
    "# Thus also inverse the treatment's correlation from negative to positive\n",
    "df.treatment = df.treatment.replace({0: 1, 1: 0})\n",
    "\n",
    "# Recalculate the treatment correlation\n",
    "print(\"✔️Treatment correlation in dataset 1 (inverted):\", correlation_treatment(df).iloc[0,0])\n",
    "\n",
    "# Add the target class feature to all three datasets\n",
    "df_inverse = declare_target_class(df)\n",
    "\n",
    "# Do the prediction process once more time\n",
    "prediction_results_inverse_1 = predict(df_inverse)\n",
    "\n",
    "# Qini evaluation results for DataSet 1 with positive treatment correlation\n",
    "ax, qini_inverse_1 = evaluation_qini(prediction_results_inverse_1)\n",
    "plt.title('Qini Curve - Dataset 1',fontsize=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
